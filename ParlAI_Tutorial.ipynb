{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsb-Cvf6lnVX"
   },
   "source": [
    "<img src=\"https://parl.ai/docs/_static/img/parlai.png\" width=\"700\"/>\n",
    "\n",
    "**Author**: Stephen Roller ([GitHub](https://github.com/stephenroller), [Twitter](https://twitter.com/stephenroller))\n",
    "\n",
    "\n",
    "# Welcome to the ParlAI interactive tutorial\n",
    "\n",
    "In this tutorial we will:\n",
    "\n",
    "- Chat with a neural network model!\n",
    "- Show how to use common commands in ParlAI, like inspecting data and model outputs.\n",
    "- See where to find information about many options.\n",
    "- Show how to fine-tune a pretrained model on a specific task\n",
    "- Add our own datasets to ParlAI\n",
    "- And add our own models to ParlAI\n",
    "\n",
    "We won't be running any examples of using Amazon Mechanical Turk, or connecting to Chat services, but you can check out our [docs](https://parl.ai/docs/) for more information on these areas.\n",
    "\n",
    "**Note:** *Make sure you're running this session with a GPU attached.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_bFnOWslsj9",
    "outputId": "6104e962-131d-42ec-f99c-59a6340f4d3b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CONDA_DEFAULT_ENV=/home/jovyan/parlai_3.10\n",
      "env: CONDA_DIR=/home/jovyan/parlai_3.10\n",
      "env: CONDA_PREFIX=/home/jovyan/parlai_3.10\n",
      "env: PYTHONPATH=/home/jovyan/parlai_3.10/bin/python\n"
     ]
    }
   ],
   "source": [
    "%env CONDA_DEFAULT_ENV /home/jovyan/parlai_3.10\n",
    "%env CONDA_DIR /home/jovyan/parlai_3.10\n",
    "%env CONDA_PREFIX /home/jovyan/parlai_3.10\n",
    "%env PYTHONPATH /home/jovyan/parlai_3.10/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] = \"/home/jovyan/parlai_3.10/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/parlai_3.10/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Additional unnecessary commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHELL=/bin/bash\n",
      "NV_LIBCUBLAS_VERSION=11.11.3.6-1\n",
      "NVIDIA_VISIBLE_DEVICES=GPU-47f2d247-e28d-0f7a-1182-f4abe887783a\n",
      "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
      "NV_NVML_DEV_VERSION=11.8.86-1\n",
      "NV_CUDNN_PACKAGE_NAME=libcudnn8\n",
      "JUPYTERHUB_ADMIN_ACCESS=1\n",
      "KUBERNETES_SERVICE_PORT=443\n",
      "PROXY_API_SERVICE_HOST=10.98.194.123\n",
      "NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8\n",
      "NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1\n",
      "HOSTNAME=jupyter-amro-2ehendawi-40fokus-2efraunhofer-2ede\n",
      "LANGUAGE=en_US.UTF-8\n",
      "JUPYTERHUB_API_TOKEN=32774cbefd9f4a2e9bf979a4bc20ce4a\n",
      "NVIDIA_REQUIRE_CUDA=cuda>=11.8 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=510,driver<511 brand=unknown,driver>=510,driver<511 brand=nvidia,driver>=510,driver<511 brand=nvidiartx,driver>=510,driver<511 brand=geforce,driver>=510,driver<511 brand=geforcertx,driver>=510,driver<511 brand=quadro,driver>=510,driver<511 brand=quadrortx,driver>=510,driver<511 brand=titan,driver>=510,driver<511 brand=titanrtx,driver>=510,driver<511 brand=tesla,driver>=515,driver<516 brand=unknown,driver>=515,driver<516 brand=nvidia,driver>=515,driver<516 brand=nvidiartx,driver>=515,driver<516 brand=geforce,driver>=515,driver<516 brand=geforcertx,driver>=515,driver<516 brand=quadro,driver>=515,driver<516 brand=quadrortx,driver>=515,driver<516 brand=titan,driver>=515,driver<516 brand=titanrtx,driver>=515,driver<516\n",
      "PROXY_API_SERVICE_PORT=8001\n",
      "NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-11-8=11.11.3.6-1\n",
      "NV_NVTX_VERSION=11.8.86-1\n",
      "JUPYTERHUB_BASE_URL=/\n",
      "NB_UID=1000\n",
      "NV_CUDA_CUDART_DEV_VERSION=11.8.89-1\n",
      "NV_LIBCUSPARSE_VERSION=11.7.5.86-1\n",
      "NV_LIBNPP_VERSION=11.8.0.86-1\n",
      "JULIA_PKGDIR=/opt/julia\n",
      "MEM_LIMIT=429496729600\n",
      "NCCL_VERSION=2.16.2-1\n",
      "PROXY_PUBLIC_PORT_80_TCP=tcp://10.103.8.203:80\n",
      "PROXY_PUBLIC_PORT=tcp://10.103.8.203:443\n",
      "PROXY_PUBLIC_SERVICE_PORT_HTTP=80\n",
      "PWD=/home/jovyan/ParlAI\n",
      "NV_CUDNN_PACKAGE=libcudnn8=8.6.0.163-1+cuda11.8\n",
      "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
      "NV_NVPROF_DEV_PACKAGE=cuda-nvprof-11-8=11.8.87-1\n",
      "MEM_GUARANTEE=8589934592\n",
      "NV_LIBNPP_PACKAGE=libnpp-11-8=11.8.0.86-1\n",
      "NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "JUPYTER_IMAGE=dockerhub.fokus.fraunhofer.de:5000/viscom/imgs/datascience-notebook-experimental:23010303\n",
      "PROXY_API_PORT_8001_TCP_ADDR=10.98.194.123\n",
      "HUB_SERVICE_HOST=10.102.170.211\n",
      "NV_LIBCUBLAS_DEV_VERSION=11.11.3.6-1\n",
      "JUPYTERHUB_SERVER_NAME=\n",
      "NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-11-8\n",
      "NV_CUDA_CUDART_VERSION=11.8.89-1\n",
      "HOME=/home/jovyan\n",
      "LANG=en_US.UTF-8\n",
      "KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443\n",
      "JPY_API_TOKEN=32774cbefd9f4a2e9bf979a4bc20ce4a\n",
      "PROXY_API_PORT_8001_TCP_PORT=8001\n",
      "HUB_SERVICE_PORT=8081\n",
      "CUDA_VERSION=11.8.0\n",
      "NB_GID=100\n",
      "NV_LIBCUBLAS_PACKAGE=libcublas-11-8=11.11.3.6-1\n",
      "JUPYTERHUB_SERVICE_PREFIX=/user/amro.hendawi@fokus.fraunhofer.de/\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "JUPYTERHUB_OAUTH_CALLBACK_URL=/user/amro.hendawi@fokus.fraunhofer.de/oauth_callback\n",
      "PROXY_PUBLIC_PORT_443_TCP=tcp://10.103.8.203:443\n",
      "CLICOLOR=1\n",
      "PROXY_PUBLIC_PORT_443_TCP_PORT=443\n",
      "NV_LIBNPP_DEV_PACKAGE=libnpp-dev-11-8=11.8.0.86-1\n",
      "PROXY_PUBLIC_SERVICE_HOST=10.103.8.203\n",
      "JUPYTERHUB_SINGLEUSER_APP=jupyter_server.serverapp.ServerApp\n",
      "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-11-8\n",
      "JULIA_VERSION=1.7.1\n",
      "PROXY_PUBLIC_PORT_80_TCP_PROTO=tcp\n",
      "HUB_PORT=tcp://10.102.170.211:8081\n",
      "XDG_CACHE_HOME=/home/jovyan/.cache/\n",
      "PROXY_PUBLIC_PORT_80_TCP_ADDR=10.103.8.203\n",
      "NV_LIBNPP_DEV_VERSION=11.8.0.86-1\n",
      "JUPYTER_IMAGE_SPEC=dockerhub.fokus.fraunhofer.de:5000/viscom/imgs/datascience-notebook-experimental:23010303\n",
      "JPY_PARENT_PID=6\n",
      "HUB_PORT_8081_TCP=tcp://10.102.170.211:8081\n",
      "TERM=xterm-color\n",
      "NV_LIBCUSPARSE_DEV_VERSION=11.7.5.86-1\n",
      "GIT_PAGER=cat\n",
      "PROXY_API_PORT=tcp://10.98.194.123:8001\n",
      "LIBRARY_PATH=/usr/local/cuda/lib64/stubs\n",
      "NV_CUDNN_VERSION=8.6.0.163\n",
      "PROXY_PUBLIC_PORT_443_TCP_PROTO=tcp\n",
      "PROXY_PUBLIC_SERVICE_PORT_HTTPS=443\n",
      "PROXY_API_PORT_8001_TCP_PROTO=tcp\n",
      "SHLVL=0\n",
      "PAGER=cat\n",
      "CONDA_DIR=/opt/conda\n",
      "NV_CUDA_LIB_VERSION=11.8.0-1\n",
      "NVARCH=x86_64\n",
      "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
      "JULIA_DEPOT_PATH=/opt/julia\n",
      "JUPYTERHUB_API_URL=http://hub:8081/hub/api\n",
      "JUPYTERHUB_CLIENT_ID=jupyterhub-user-amro.hendawi%40fokus.fraunhofer.de\n",
      "CUDNN_VERSION=8.1.1.33\n",
      "PROXY_PUBLIC_PORT_80_TCP_PORT=80\n",
      "NV_CUDNN_PACKAGE_DEV=libcudnn8-dev=8.6.0.163-1+cuda11.8\n",
      "KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1\n",
      "JUPYTERHUB_HOST=\n",
      "NV_CUDA_COMPAT_PACKAGE=cuda-compat-11-8\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "PROXY_PUBLIC_PORT_443_TCP_ADDR=10.103.8.203\n",
      "NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8\n",
      "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "HUB_SERVICE_PORT_HUB=8081\n",
      "GIT_PYTHON_REFRESH=quiet\n",
      "NB_USER=jovyan\n",
      "KUBERNETES_SERVICE_HOST=10.96.0.1\n",
      "NV_NVPROF_VERSION=11.8.87-1\n",
      "LC_ALL=en_US.UTF-8\n",
      "KUBERNETES_PORT=tcp://10.96.0.1:443\n",
      "KUBERNETES_PORT_443_TCP_PORT=443\n",
      "PROXY_API_PORT_8001_TCP=tcp://10.98.194.123:8001\n",
      "PROXY_PUBLIC_SERVICE_PORT=443\n",
      "HUB_PORT_8081_TCP_PORT=8081\n",
      "PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "HUB_PORT_8081_TCP_ADDR=10.102.170.211\n",
      "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "HUB_PORT_8081_TCP_PROTO=tcp\n",
      "NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1\n",
      "JUPYTERHUB_USER=amro.hendawi@fokus.fraunhofer.de\n",
      "JUPYTERHUB_ACTIVITY_URL=http://hub:8081/hub/api/users/amro.hendawi@fokus.fraunhofer.de/activity\n",
      "DEBIAN_FRONTEND=noninteractive\n",
      "_=/usr/bin/printenv\n"
     ]
    }
   ],
   "source": [
    "!printenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMxd1KIRl9Xm"
   },
   "source": [
    "## Installing parlai\n",
    "\n",
    "We need to install ParlAI. Since we're in Google Colab, we can assume PyTorch and similar dependencies are installed already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "i93Mn_I7MOEO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "18dff4ad-f0f0-4aa4-e964-0f3e3b22d45e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.5 MB 10.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 35.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 285 kB 52.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 8.0 MB 31.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 48 kB 5.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 38.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 547 kB 38.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 35.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 138 kB 53.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 74 kB 3.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 235 kB 52.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 208 kB 45.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 145 kB 49.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 180 kB 49.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 190 kB 43.5 MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 124 kB 51.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 170 kB 49.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 748 kB 26.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 131 kB 50.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 40 kB 5.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 38.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 125 kB 52.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 56 kB 5.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 243 kB 52.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 112 kB 47.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 636 kB 32.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 110 kB 51.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 62 kB 792 kB/s \n",
      "\u001b[K     |████████████████████████████████| 100 kB 9.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 84 kB 3.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 121 kB 54.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 90 kB 10.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 160 kB 51.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 271 kB 50.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 86 kB 5.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 69 kB 6.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 42 kB 890 kB/s \n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
      "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q parlai\n",
    "!pip install -q subword_nmt # extra requirement we need for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtVz5dCUmFkN"
   },
   "source": [
    "# Chatting with a model\n",
    "\n",
    "Let's start by chatting interactively with a model file from our model zoo! We'll pick our \"tutorial transformer generator\" model, which is a generative transformer trained on pushshift.io Reddit. You can take a look at the [model zoo](https://parl.ai/docs/zoo.html) for a more complete list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nRJGRtMKmIWV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4e286328-fe89-4b44-8e0d-1ab1cf5ac656",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:51:19 | \u001b[33mOverriding opt[\"model_file\"] to /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model (previously: /checkpoint/roller/20190909/cleanreddit/585/model)\u001b[0m\n",
      "08:51:19 | loading dictionary from /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model.dict\n",
      "08:51:19 | num words = 54944\n",
      "08:51:19 | TransformerGenerator: full interactive mode on.\n",
      "08:51:21 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "08:51:21 | Loading existing model params from /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model\n",
      "08:51:23 | Opt:\n",
      "08:51:23 |     activation: gelu\n",
      "08:51:23 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "08:51:23 |     adam_eps: 1e-06\n",
      "08:51:23 |     add_p1_after_newln: False\n",
      "08:51:23 |     aggregate_micro: False\n",
      "08:51:23 |     allow_missing_init_opts: False\n",
      "08:51:23 |     attention_dropout: 0.0\n",
      "08:51:23 |     batch_length_range: 5\n",
      "08:51:23 |     batch_sort_cache_type: pop\n",
      "08:51:23 |     batch_sort_field: text\n",
      "08:51:23 |     batchsize: 48\n",
      "08:51:23 |     beam_block_full_context: False\n",
      "08:51:23 |     beam_block_list_filename: None\n",
      "08:51:23 |     beam_block_ngram: 3\n",
      "08:51:23 |     beam_context_block_ngram: 3\n",
      "08:51:23 |     beam_delay: 30\n",
      "08:51:23 |     beam_length_penalty: 0.65\n",
      "08:51:23 |     beam_min_length: 10\n",
      "08:51:23 |     beam_min_n_best: 3\n",
      "08:51:23 |     beam_size: 8\n",
      "08:51:23 |     betas: '[0.9, 0.98]'\n",
      "08:51:23 |     bpe_add_prefix_space: None\n",
      "08:51:23 |     bpe_debug: False\n",
      "08:51:23 |     bpe_dropout: None\n",
      "08:51:23 |     bpe_merge: None\n",
      "08:51:23 |     bpe_vocab: None\n",
      "08:51:23 |     checkpoint_activations: False\n",
      "08:51:23 |     compute_tokenized_bleu: False\n",
      "08:51:23 |     datapath: /home/jovyan/ParlAI/data\n",
      "08:51:23 |     datatype: train:stream\n",
      "08:51:23 |     delimiter: '\\n'\n",
      "08:51:23 |     dict_build_first: True\n",
      "08:51:23 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "08:51:23 |     dict_endtoken: __end__\n",
      "08:51:23 |     dict_file: /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model.dict\n",
      "08:51:23 |     dict_include_test: False\n",
      "08:51:23 |     dict_include_valid: False\n",
      "08:51:23 |     dict_initpath: None\n",
      "08:51:23 |     dict_language: english\n",
      "08:51:23 |     dict_loaded: True\n",
      "08:51:23 |     dict_lower: True\n",
      "08:51:23 |     dict_max_ngram_size: -1\n",
      "08:51:23 |     dict_maxexs: -1\n",
      "08:51:23 |     dict_maxtokens: -1\n",
      "08:51:23 |     dict_minfreq: 0\n",
      "08:51:23 |     dict_nulltoken: __null__\n",
      "08:51:23 |     dict_starttoken: __start__\n",
      "08:51:23 |     dict_textfields: text,labels\n",
      "08:51:23 |     dict_tokenizer: bpe\n",
      "08:51:23 |     dict_unktoken: __unk__\n",
      "08:51:23 |     display_add_fields: \n",
      "08:51:23 |     display_examples: False\n",
      "08:51:23 |     display_prettify: False\n",
      "08:51:23 |     distributed_world_size: 64\n",
      "08:51:23 |     download_path: None\n",
      "08:51:23 |     dropout: 0.1\n",
      "08:51:23 |     dynamic_batching: None\n",
      "08:51:23 |     embedding_projection: random\n",
      "08:51:23 |     embedding_size: 512\n",
      "08:51:23 |     embedding_type: random\n",
      "08:51:23 |     embeddings_scale: True\n",
      "08:51:23 |     eval_batchsize: None\n",
      "08:51:23 |     evaltask: None\n",
      "08:51:23 |     ffn_size: 2048\n",
      "08:51:23 |     force_fp16_tokens: True\n",
      "08:51:23 |     fp16: True\n",
      "08:51:23 |     fp16_impl: safe\n",
      "08:51:23 |     gpu: 0\n",
      "08:51:23 |     gpu_beam_blocking: False\n",
      "08:51:23 |     gradient_clip: 10.0\n",
      "08:51:23 |     hide_labels: False\n",
      "08:51:23 |     history_add_global_end_token: None\n",
      "08:51:23 |     history_reversed: False\n",
      "08:51:23 |     history_size: -1\n",
      "08:51:23 |     image_cropsize: 224\n",
      "08:51:23 |     image_mode: raw\n",
      "08:51:23 |     image_size: 256\n",
      "08:51:23 |     inference: beam\n",
      "08:51:23 |     init_model: None\n",
      "08:51:23 |     init_opt: None\n",
      "08:51:23 |     interactive_mode: True\n",
      "08:51:23 |     interactive_task: True\n",
      "08:51:23 |     invsqrt_lr_decay_gamma: -1\n",
      "08:51:23 |     is_debug: False\n",
      "08:51:23 |     label_truncate: 128\n",
      "08:51:23 |     lambda_decay: 0.9\n",
      "08:51:23 |     learn_positional_embeddings: True\n",
      "08:51:23 |     learningrate: 0.0005\n",
      "08:51:23 |     local_human_candidates_file: None\n",
      "08:51:23 |     log_every_n_secs: 30.0\n",
      "08:51:23 |     log_keep_fields: all\n",
      "08:51:23 |     loglevel: info\n",
      "08:51:23 |     lr_scheduler: invsqrt\n",
      "08:51:23 |     lr_scheduler_decay: 0.5\n",
      "08:51:23 |     lr_scheduler_patience: 3\n",
      "08:51:23 |     max_train_time: -1\n",
      "08:51:23 |     metrics: default\n",
      "08:51:23 |     model: transformer/generator\n",
      "08:51:23 |     model_file: /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model\n",
      "08:51:23 |     model_parallel: False\n",
      "08:51:23 |     momentum: 0\n",
      "08:51:23 |     multitask_weights: [1]\n",
      "08:51:23 |     n_decoder_layers: -1\n",
      "08:51:23 |     n_encoder_layers: -1\n",
      "08:51:23 |     n_heads: 16\n",
      "08:51:23 |     n_layers: 8\n",
      "08:51:23 |     n_positions: 512\n",
      "08:51:23 |     n_segments: 0\n",
      "08:51:23 |     nesterov: True\n",
      "08:51:23 |     no_cuda: False\n",
      "08:51:23 |     num_epochs: 5.0\n",
      "08:51:23 |     numthreads: 1\n",
      "08:51:23 |     numworkers: 4\n",
      "08:51:23 |     nus: [0.7]\n",
      "08:51:23 |     omega_bound: 0.3\n",
      "08:51:23 |     optimizer: fused_adam\n",
      "08:51:23 |     outfile: \n",
      "08:51:23 |     output_scaling: 1.0\n",
      "08:51:23 |     override: \"{'model_file': '/home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model'}\"\n",
      "08:51:23 |     p_reset: True\n",
      "08:51:23 |     parlai_home: /home/jovyan/ParlAI\n",
      "08:51:23 |     person_tokens: False\n",
      "08:51:23 |     port: 61337\n",
      "08:51:23 |     pytorch_context_length: -1\n",
      "08:51:23 |     pytorch_datapath: None\n",
      "08:51:23 |     pytorch_include_labels: True\n",
      "08:51:23 |     pytorch_preprocess: False\n",
      "08:51:23 |     pytorch_teacher_batch_sort: False\n",
      "08:51:23 |     pytorch_teacher_dataset: None\n",
      "08:51:23 |     pytorch_teacher_task: None\n",
      "08:51:23 |     rank_candidates: False\n",
      "08:51:23 |     relu_dropout: 0.0\n",
      "08:51:23 |     save_after_valid: True\n",
      "08:51:23 |     save_every_n_secs: -1\n",
      "08:51:23 |     save_format: conversations\n",
      "08:51:23 |     share_word_embeddings: True\n",
      "08:51:23 |     short_final_eval: True\n",
      "08:51:23 |     show_advanced_args: False\n",
      "08:51:23 |     shuffle: False\n",
      "08:51:23 |     single_turn: False\n",
      "08:51:23 |     skip_generation: False\n",
      "08:51:23 |     special_tok_lst: None\n",
      "08:51:23 |     split_lines: False\n",
      "08:51:23 |     starttime: Jan27_08-51\n",
      "08:51:23 |     task: internal:new_reddit:presorted\n",
      "08:51:23 |     temperature: 1.0\n",
      "08:51:23 |     tensorboard_log: False\n",
      "08:51:23 |     text_truncate: 512\n",
      "08:51:23 |     topk: 10\n",
      "08:51:23 |     topp: 0.9\n",
      "08:51:23 |     truncate: -1\n",
      "08:51:23 |     update_freq: 1\n",
      "08:51:23 |     use_reply: label\n",
      "08:51:23 |     validation_cutoff: 1.0\n",
      "08:51:23 |     validation_every_n_epochs: -1\n",
      "08:51:23 |     validation_every_n_secs: 1800.0\n",
      "08:51:23 |     validation_max_exs: 9920\n",
      "08:51:23 |     validation_metric: ppl\n",
      "08:51:23 |     validation_metric_mode: min\n",
      "08:51:23 |     validation_patience: 0\n",
      "08:51:23 |     validation_share_agent: False\n",
      "08:51:23 |     variant: xlm\n",
      "08:51:23 |     verbose: False\n",
      "08:51:23 |     warmup_rate: 0.0001\n",
      "08:51:23 |     warmup_updates: 20000\n",
      "08:51:23 |     weight_decay: 0.01\n",
      "08:51:23 | Current ParlAI commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "08:51:23 | Current internal commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "08:51:23 | Current fb commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
      "08:51:23 | creating task(s): interactive\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[0mEnter Your Message:\u001b[0;0m  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1m+ / u / dogetipbot 10 doge\u001b[0;0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[0mEnter Your Message:\u001b[0;0m  [EXIT]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAT DONE \n"
     ]
    }
   ],
   "source": [
    "# Import the Interactive script\n",
    "from parlai.scripts.interactive import Interactive\n",
    "\n",
    "# call it with particular args\n",
    "Interactive.main(\n",
    "    # the model_file is a filename path pointing to a particular model dump.\n",
    "    # Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n",
    "    # They'll be automatically downloaded when you ask to use them.\n",
    "    model_file='zoo:tutorial_transformer_generator/model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hfUEgovmWay"
   },
   "source": [
    "The same on the command line:\n",
    "```bash\n",
    "python -m parlai.scripts.interactive --model-file zoo:tutorial_transformer_generator/model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_hGrZGGmaWF"
   },
   "source": [
    "# Taking a look at some data\n",
    "\n",
    "We can look at look into a specific dataset. Let's look into the \"empathetic dialogues\" dataset, which aims to teach models how to respond with text expressing the appropriate emotion. We have over existing 80 datasets in ParlAI. You can take a full look in our [task list](https://parl.ai/docs/tasks.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "AqckSXqlmWuT",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e45fd031-547e-4389-c7c9-57819bfbbca0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:53:48 | Opt:\n",
      "08:53:48 |     allow_missing_init_opts: False\n",
      "08:53:48 |     batchsize: 1\n",
      "08:53:48 |     datapath: /home/jovyan/ParlAI/data\n",
      "08:53:48 |     datatype: train:ordered\n",
      "08:53:48 |     dict_class: None\n",
      "08:53:48 |     display_add_fields: \n",
      "08:53:48 |     download_path: None\n",
      "08:53:48 |     dynamic_batching: None\n",
      "08:53:48 |     hide_labels: False\n",
      "08:53:48 |     ignore_agent_reply: True\n",
      "08:53:48 |     image_cropsize: 224\n",
      "08:53:48 |     image_mode: raw\n",
      "08:53:48 |     image_size: 256\n",
      "08:53:48 |     init_model: None\n",
      "08:53:48 |     init_opt: None\n",
      "08:53:48 |     is_debug: False\n",
      "08:53:48 |     loglevel: info\n",
      "08:53:48 |     max_display_len: 1000\n",
      "08:53:48 |     model: None\n",
      "08:53:48 |     model_file: None\n",
      "08:53:48 |     multitask_weights: [1]\n",
      "08:53:48 |     mutators: None\n",
      "08:53:48 |     num_examples: 5\n",
      "08:53:48 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 5}\"\n",
      "08:53:48 |     parlai_home: /home/jovyan/ParlAI\n",
      "08:53:48 |     starttime: Jan27_08-53\n",
      "08:53:48 |     task: empathetic_dialogues\n",
      "08:53:48 |     teacher_seed: None\n",
      "08:53:48 |     train_experiencer_only: False\n",
      "08:53:48 |     verbose: False\n",
      "08:53:48 | Current ParlAI commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "08:53:48 | Current internal commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "08:53:48 | Current fb commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "08:53:48 | creating task(s): empathetic_dialogues\n",
      "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
      "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
      "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
      "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
      "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
      "\u001b[0mWe no longer talk.\u001b[0;0m\n",
      "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
      "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
      "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
      "\u001b[0mWhere has she gone?\u001b[0;0m\n",
      "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
      "08:53:48 | loaded 39057 episodes with a total of 64636 examples\n"
     ]
    }
   ],
   "source": [
    "# The display_data script is used to show the contents of a particular task.\n",
    "# By default, we show the train\n",
    "from parlai.scripts.display_data import DisplayData\n",
    "DisplayData.main(task='empathetic_dialogues', num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9C6oHq87zGx"
   },
   "source": [
    "The black, unindented text is the _prompt_, while the blue text is the _label_. That is, the label is what we will be training the model to mimic.\n",
    "\n",
    "We can also ask to see fewer examples, and get them from the validation set instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "RGNSBetWmfGF",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5e13563e-a749-49f6-b6b0-19d2c7473448",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:53:50 | Opt:\n",
      "08:53:50 |     allow_missing_init_opts: False\n",
      "08:53:50 |     batchsize: 1\n",
      "08:53:50 |     datapath: /home/jovyan/ParlAI/data\n",
      "08:53:50 |     datatype: valid\n",
      "08:53:50 |     dict_class: None\n",
      "08:53:50 |     display_add_fields: \n",
      "08:53:50 |     download_path: None\n",
      "08:53:50 |     dynamic_batching: None\n",
      "08:53:50 |     hide_labels: False\n",
      "08:53:50 |     ignore_agent_reply: True\n",
      "08:53:50 |     image_cropsize: 224\n",
      "08:53:50 |     image_mode: raw\n",
      "08:53:50 |     image_size: 256\n",
      "08:53:50 |     init_model: None\n",
      "08:53:50 |     init_opt: None\n",
      "08:53:50 |     is_debug: False\n",
      "08:53:50 |     loglevel: info\n",
      "08:53:50 |     max_display_len: 1000\n",
      "08:53:50 |     model: None\n",
      "08:53:50 |     model_file: None\n",
      "08:53:50 |     multitask_weights: [1]\n",
      "08:53:50 |     mutators: None\n",
      "08:53:50 |     num_examples: 3\n",
      "08:53:50 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 3, 'datatype': 'valid'}\"\n",
      "08:53:50 |     parlai_home: /home/jovyan/ParlAI\n",
      "08:53:50 |     starttime: Jan27_08-53\n",
      "08:53:50 |     task: empathetic_dialogues\n",
      "08:53:50 |     teacher_seed: None\n",
      "08:53:50 |     train_experiencer_only: False\n",
      "08:53:50 |     verbose: False\n",
      "08:53:50 | Current ParlAI commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "08:53:50 | Current internal commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "08:53:50 | Current fb commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "08:53:50 | creating task(s): empathetic_dialogues\n",
      "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
      "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
      "   \u001b[1;94mAre you fine now?\u001b[0;0m\n",
      "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
      "   \u001b[1;94mCool :) Is your car damaged a lot?\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
      "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
      "   \u001b[1;94mThat's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
      "08:53:50 | loaded 2769 episodes with a total of 5738 examples\n"
     ]
    }
   ],
   "source": [
    "# we can instead ask to see fewer examples, and get them from the valid set.\n",
    "DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVSrgRrEmdS-"
   },
   "source": [
    "On the command line:\n",
    "```bash\n",
    "python -m parlai.scripts.display_data --task empathetic_dialogues\n",
    "```\n",
    "or a bit shorter\n",
    "```\n",
    "python -m parlai.scripts.display_data -t empathetic_dialogues\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_M8Zr86n2_G"
   },
   "source": [
    "# Training a model\n",
    "\n",
    "Well it's one thing looking at data, but what if we want to train our own model (from scratch)? Let's train a very simple seq2seq LSTM with attention, to respond to empathetic dialogues.\n",
    "\n",
    "To get some extra performance, we'll initialize using GloVe embeddings, but we will cap the training time to 2 minutes for this tutorial. It won't perform very well, but that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "pBhVQycSn2q_",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e424398c-1dc2-4bde-b723-6b2df62211cb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:49:39 | building dictionary first...\n",
      "18:49:39 | Opt:\n",
      "18:49:39 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "18:49:39 |     adam_eps: 1e-08\n",
      "18:49:39 |     add_p1_after_newln: False\n",
      "18:49:39 |     aggregate_micro: False\n",
      "18:49:39 |     allow_missing_init_opts: False\n",
      "18:49:39 |     attention: dot\n",
      "18:49:39 |     attention_length: 48\n",
      "18:49:39 |     attention_time: post\n",
      "18:49:39 |     batchsize: 1\n",
      "18:49:39 |     beam_block_full_context: True\n",
      "18:49:39 |     beam_block_list_filename: None\n",
      "18:49:39 |     beam_block_ngram: -1\n",
      "18:49:39 |     beam_context_block_ngram: -1\n",
      "18:49:39 |     beam_delay: 30\n",
      "18:49:39 |     beam_length_penalty: 0.65\n",
      "18:49:39 |     beam_min_length: 1\n",
      "18:49:39 |     beam_size: 1\n",
      "18:49:39 |     betas: '(0.9, 0.999)'\n",
      "18:49:39 |     bidirectional: False\n",
      "18:49:39 |     bpe_add_prefix_space: None\n",
      "18:49:39 |     bpe_debug: False\n",
      "18:49:39 |     bpe_dropout: None\n",
      "18:49:39 |     bpe_merge: None\n",
      "18:49:39 |     bpe_vocab: None\n",
      "18:49:39 |     compute_tokenized_bleu: False\n",
      "18:49:39 |     datapath: /home/jovyan/ParlAI/data\n",
      "18:49:39 |     datatype: train\n",
      "18:49:39 |     decoder: same\n",
      "18:49:39 |     delimiter: '\\n'\n",
      "18:49:39 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "18:49:39 |     dict_endtoken: __end__\n",
      "18:49:39 |     dict_file: from_scratch_model/model.dict\n",
      "18:49:39 |     dict_include_test: False\n",
      "18:49:39 |     dict_include_valid: False\n",
      "18:49:39 |     dict_initpath: None\n",
      "18:49:39 |     dict_language: english\n",
      "18:49:39 |     dict_loaded: False\n",
      "18:49:39 |     dict_lower: False\n",
      "18:49:39 |     dict_max_ngram_size: -1\n",
      "18:49:39 |     dict_maxexs: -1\n",
      "18:49:39 |     dict_maxtokens: -1\n",
      "18:49:39 |     dict_minfreq: 0\n",
      "18:49:39 |     dict_nulltoken: __null__\n",
      "18:49:39 |     dict_starttoken: __start__\n",
      "18:49:39 |     dict_textfields: text,labels\n",
      "18:49:39 |     dict_tokenizer: re\n",
      "18:49:39 |     dict_unktoken: __unk__\n",
      "18:49:39 |     display_examples: False\n",
      "18:49:39 |     download_path: None\n",
      "18:49:39 |     dropout: 0.1\n",
      "18:49:39 |     dynamic_batching: None\n",
      "18:49:39 |     embedding_projection: random\n",
      "18:49:39 |     embedding_type: random\n",
      "18:49:39 |     embeddingsize: 128\n",
      "18:49:39 |     eval_batchsize: None\n",
      "18:49:39 |     eval_dynamic_batching: None\n",
      "18:49:39 |     evaltask: None\n",
      "18:49:39 |     final_extra_opt: \n",
      "18:49:39 |     force_fp16_tokens: False\n",
      "18:49:39 |     fp16: False\n",
      "18:49:39 |     fp16_impl: safe\n",
      "18:49:39 |     gpu: -1\n",
      "18:49:39 |     gpu_beam_blocking: False\n",
      "18:49:39 |     gradient_clip: 0.1\n",
      "18:49:39 |     hiddensize: 128\n",
      "18:49:39 |     hide_labels: False\n",
      "18:49:39 |     history_add_global_end_token: None\n",
      "18:49:39 |     history_reversed: False\n",
      "18:49:39 |     history_size: -1\n",
      "18:49:39 |     image_cropsize: 224\n",
      "18:49:39 |     image_mode: no_image_model\n",
      "18:49:39 |     image_size: 256\n",
      "18:49:39 |     inference: greedy\n",
      "18:49:39 |     init_model: None\n",
      "18:49:39 |     init_opt: None\n",
      "18:49:39 |     input_dropout: 0.0\n",
      "18:49:39 |     interactive_mode: False\n",
      "18:49:39 |     invsqrt_lr_decay_gamma: -1\n",
      "18:49:39 |     is_debug: False\n",
      "18:49:39 |     label_truncate: None\n",
      "18:49:39 |     lambda_decay: 0.9\n",
      "18:49:39 |     learningrate: 1\n",
      "18:49:39 |     load_from_checkpoint: True\n",
      "18:49:39 |     log_every_n_secs: -1\n",
      "18:49:39 |     log_every_n_steps: 50\n",
      "18:49:39 |     log_keep_fields: all\n",
      "18:49:39 |     loglevel: info\n",
      "18:49:39 |     lookuptable: all\n",
      "18:49:39 |     lr_scheduler: reduceonplateau\n",
      "18:49:39 |     lr_scheduler_decay: 0.5\n",
      "18:49:39 |     lr_scheduler_patience: 3\n",
      "18:49:39 |     max_train_steps: -1\n",
      "18:49:39 |     max_train_time: 120.0\n",
      "18:49:39 |     metrics: default\n",
      "18:49:39 |     model: seq2seq\n",
      "18:49:39 |     model_file: from_scratch_model/model\n",
      "18:49:39 |     momentum: 0\n",
      "18:49:39 |     multitask_weights: [1]\n",
      "18:49:39 |     mutators: None\n",
      "18:49:39 |     nesterov: True\n",
      "18:49:39 |     no_cuda: False\n",
      "18:49:39 |     num_epochs: -1\n",
      "18:49:39 |     num_workers: 0\n",
      "18:49:39 |     numlayers: 2\n",
      "18:49:39 |     numsoftmax: 1\n",
      "18:49:39 |     nus: (0.7,)\n",
      "18:49:39 |     omega_bound: 0.3\n",
      "18:49:39 |     optimizer: sgd\n",
      "18:49:39 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
      "18:49:39 |     p_reset: True\n",
      "18:49:39 |     parlai_home: /home/jovyan/ParlAI\n",
      "18:49:39 |     person_tokens: False\n",
      "18:49:39 |     rank_candidates: False\n",
      "18:49:39 |     rnn_class: lstm\n",
      "18:49:39 |     save_after_valid: False\n",
      "18:49:39 |     save_every_n_secs: -1\n",
      "18:49:39 |     save_format: conversations\n",
      "18:49:39 |     seed: None\n",
      "18:49:39 |     short_final_eval: False\n",
      "18:49:39 |     skip_generation: False\n",
      "18:49:39 |     special_tok_lst: None\n",
      "18:49:39 |     split_lines: False\n",
      "18:49:39 |     starttime: Jan26_18-49\n",
      "18:49:39 |     task: empathetic_dialogues\n",
      "18:49:39 |     teacher_seed: None\n",
      "18:49:39 |     temperature: 1.0\n",
      "18:49:39 |     tensorboard_log: False\n",
      "18:49:39 |     tensorboard_logdir: None\n",
      "18:49:39 |     text_truncate: None\n",
      "18:49:39 |     topk: 10\n",
      "18:49:39 |     topp: 0.9\n",
      "18:49:39 |     train_experiencer_only: False\n",
      "18:49:39 |     truncate: 64\n",
      "18:49:39 |     update_freq: 1\n",
      "18:49:39 |     use_reply: label\n",
      "18:49:39 |     validation_cutoff: 1.0\n",
      "18:49:39 |     validation_every_n_epochs: -1\n",
      "18:49:39 |     validation_every_n_secs: -1\n",
      "18:49:39 |     validation_every_n_steps: -1\n",
      "18:49:39 |     validation_max_exs: -1\n",
      "18:49:39 |     validation_metric: accuracy\n",
      "18:49:39 |     validation_metric_mode: None\n",
      "18:49:39 |     validation_patience: 10\n",
      "18:49:39 |     validation_share_agent: False\n",
      "18:49:39 |     verbose: False\n",
      "18:49:39 |     wandb_entity: None\n",
      "18:49:39 |     wandb_log: False\n",
      "18:49:39 |     wandb_log_model: False\n",
      "18:49:39 |     wandb_name: None\n",
      "18:49:39 |     wandb_project: None\n",
      "18:49:39 |     warmup_rate: 0.0001\n",
      "18:49:39 |     warmup_updates: -1\n",
      "18:49:39 |     weight_decay: None\n",
      "18:49:39 |     world_logs: \n",
      "18:49:39 | Current ParlAI commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:49:39 | Current internal commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:49:39 | Current fb commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:49:39 | creating task(s): empathetic_dialogues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dictionary: 100%|██████████| 64.6k/64.6k [00:03<00:00, 20.3kex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:49:43 | Saving dictionary to from_scratch_model/model.dict\n",
      "18:49:43 | dictionary built with 22419 tokens in 0.0s\n",
      "18:49:43 | No model with opt yet at: from_scratch_model/model(.opt)\n",
      "18:49:43 | loading dictionary from from_scratch_model/model.dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:49:43 | num words = 22419\n",
      "18:49:43 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
      "18:49:43 | Opt:\n",
      "18:49:43 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "18:49:43 |     adam_eps: 1e-08\n",
      "18:49:43 |     add_p1_after_newln: False\n",
      "18:49:43 |     aggregate_micro: False\n",
      "18:49:43 |     allow_missing_init_opts: False\n",
      "18:49:43 |     attention: dot\n",
      "18:49:43 |     attention_length: 48\n",
      "18:49:43 |     attention_time: post\n",
      "18:49:43 |     batchsize: 16\n",
      "18:49:43 |     beam_block_full_context: True\n",
      "18:49:43 |     beam_block_list_filename: None\n",
      "18:49:43 |     beam_block_ngram: -1\n",
      "18:49:43 |     beam_context_block_ngram: -1\n",
      "18:49:43 |     beam_delay: 30\n",
      "18:49:43 |     beam_length_penalty: 0.65\n",
      "18:49:43 |     beam_min_length: 1\n",
      "18:49:43 |     beam_size: 1\n",
      "18:49:43 |     betas: '(0.9, 0.999)'\n",
      "18:49:43 |     bidirectional: False\n",
      "18:49:43 |     bpe_add_prefix_space: None\n",
      "18:49:43 |     bpe_debug: False\n",
      "18:49:43 |     bpe_dropout: None\n",
      "18:49:43 |     bpe_merge: None\n",
      "18:49:43 |     bpe_vocab: None\n",
      "18:49:43 |     compute_tokenized_bleu: False\n",
      "18:49:43 |     datapath: /home/jovyan/ParlAI/data\n",
      "18:49:43 |     datatype: train\n",
      "18:49:43 |     decoder: same\n",
      "18:49:43 |     delimiter: '\\n'\n",
      "18:49:43 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "18:49:43 |     dict_endtoken: __end__\n",
      "18:49:43 |     dict_file: from_scratch_model/model.dict\n",
      "18:49:43 |     dict_include_test: False\n",
      "18:49:43 |     dict_include_valid: False\n",
      "18:49:43 |     dict_initpath: None\n",
      "18:49:43 |     dict_language: english\n",
      "18:49:43 |     dict_loaded: True\n",
      "18:49:43 |     dict_lower: False\n",
      "18:49:43 |     dict_max_ngram_size: -1\n",
      "18:49:43 |     dict_maxexs: -1\n",
      "18:49:43 |     dict_maxtokens: -1\n",
      "18:49:43 |     dict_minfreq: 0\n",
      "18:49:43 |     dict_nulltoken: __null__\n",
      "18:49:43 |     dict_starttoken: __start__\n",
      "18:49:43 |     dict_textfields: text,labels\n",
      "18:49:43 |     dict_tokenizer: re\n",
      "18:49:43 |     dict_unktoken: __unk__\n",
      "18:49:43 |     display_examples: False\n",
      "18:49:43 |     download_path: None\n",
      "18:49:43 |     dropout: 0.1\n",
      "18:49:43 |     dynamic_batching: None\n",
      "18:49:43 |     embedding_projection: random\n",
      "18:49:43 |     embedding_type: random\n",
      "18:49:43 |     embeddingsize: 128\n",
      "18:49:43 |     eval_batchsize: None\n",
      "18:49:43 |     eval_dynamic_batching: None\n",
      "18:49:43 |     evaltask: None\n",
      "18:49:43 |     final_extra_opt: \n",
      "18:49:43 |     force_fp16_tokens: False\n",
      "18:49:43 |     fp16: False\n",
      "18:49:43 |     fp16_impl: safe\n",
      "18:49:43 |     gpu: -1\n",
      "18:49:43 |     gpu_beam_blocking: False\n",
      "18:49:43 |     gradient_clip: 0.1\n",
      "18:49:43 |     hiddensize: 128\n",
      "18:49:43 |     hide_labels: False\n",
      "18:49:43 |     history_add_global_end_token: None\n",
      "18:49:43 |     history_reversed: False\n",
      "18:49:43 |     history_size: -1\n",
      "18:49:43 |     image_cropsize: 224\n",
      "18:49:43 |     image_mode: raw\n",
      "18:49:43 |     image_size: 256\n",
      "18:49:43 |     inference: greedy\n",
      "18:49:43 |     init_model: None\n",
      "18:49:43 |     init_opt: None\n",
      "18:49:43 |     input_dropout: 0.0\n",
      "18:49:43 |     interactive_mode: False\n",
      "18:49:43 |     invsqrt_lr_decay_gamma: -1\n",
      "18:49:43 |     is_debug: False\n",
      "18:49:43 |     label_truncate: None\n",
      "18:49:43 |     lambda_decay: 0.9\n",
      "18:49:43 |     learningrate: 1\n",
      "18:49:43 |     load_from_checkpoint: True\n",
      "18:49:43 |     log_every_n_secs: -1\n",
      "18:49:43 |     log_every_n_steps: 50\n",
      "18:49:43 |     log_keep_fields: all\n",
      "18:49:43 |     loglevel: info\n",
      "18:49:43 |     lookuptable: all\n",
      "18:49:43 |     lr_scheduler: reduceonplateau\n",
      "18:49:43 |     lr_scheduler_decay: 0.5\n",
      "18:49:43 |     lr_scheduler_patience: 3\n",
      "18:49:43 |     max_train_steps: -1\n",
      "18:49:43 |     max_train_time: 120.0\n",
      "18:49:43 |     metrics: default\n",
      "18:49:43 |     model: seq2seq\n",
      "18:49:43 |     model_file: from_scratch_model/model\n",
      "18:49:43 |     momentum: 0\n",
      "18:49:43 |     multitask_weights: [1]\n",
      "18:49:43 |     mutators: None\n",
      "18:49:43 |     nesterov: True\n",
      "18:49:43 |     no_cuda: False\n",
      "18:49:43 |     num_epochs: -1\n",
      "18:49:43 |     num_workers: 0\n",
      "18:49:43 |     numlayers: 2\n",
      "18:49:43 |     numsoftmax: 1\n",
      "18:49:43 |     nus: (0.7,)\n",
      "18:49:43 |     omega_bound: 0.3\n",
      "18:49:43 |     optimizer: sgd\n",
      "18:49:43 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
      "18:49:43 |     p_reset: True\n",
      "18:49:43 |     parlai_home: /home/jovyan/ParlAI\n",
      "18:49:43 |     person_tokens: False\n",
      "18:49:43 |     rank_candidates: False\n",
      "18:49:43 |     rnn_class: lstm\n",
      "18:49:43 |     save_after_valid: False\n",
      "18:49:43 |     save_every_n_secs: -1\n",
      "18:49:43 |     save_format: conversations\n",
      "18:49:43 |     seed: None\n",
      "18:49:43 |     short_final_eval: False\n",
      "18:49:43 |     skip_generation: False\n",
      "18:49:43 |     special_tok_lst: None\n",
      "18:49:43 |     split_lines: False\n",
      "18:49:43 |     starttime: Jan26_18-49\n",
      "18:49:43 |     task: empathetic_dialogues\n",
      "18:49:43 |     teacher_seed: None\n",
      "18:49:43 |     temperature: 1.0\n",
      "18:49:43 |     tensorboard_log: False\n",
      "18:49:43 |     tensorboard_logdir: None\n",
      "18:49:43 |     text_truncate: None\n",
      "18:49:43 |     topk: 10\n",
      "18:49:43 |     topp: 0.9\n",
      "18:49:43 |     train_experiencer_only: False\n",
      "18:49:43 |     truncate: 64\n",
      "18:49:43 |     update_freq: 1\n",
      "18:49:43 |     use_reply: label\n",
      "18:49:43 |     validation_cutoff: 1.0\n",
      "18:49:43 |     validation_every_n_epochs: -1\n",
      "18:49:43 |     validation_every_n_secs: -1\n",
      "18:49:43 |     validation_every_n_steps: -1\n",
      "18:49:43 |     validation_max_exs: -1\n",
      "18:49:43 |     validation_metric: accuracy\n",
      "18:49:43 |     validation_metric_mode: None\n",
      "18:49:43 |     validation_patience: 10\n",
      "18:49:43 |     validation_share_agent: False\n",
      "18:49:43 |     verbose: False\n",
      "18:49:43 |     wandb_entity: None\n",
      "18:49:43 |     wandb_log: False\n",
      "18:49:43 |     wandb_log_model: False\n",
      "18:49:43 |     wandb_name: None\n",
      "18:49:43 |     wandb_project: None\n",
      "18:49:43 |     warmup_rate: 0.0001\n",
      "18:49:43 |     warmup_updates: -1\n",
      "18:49:43 |     weight_decay: None\n",
      "18:49:43 |     world_logs: \n",
      "18:49:43 | Current ParlAI commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:49:43 | Current internal commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:49:43 | Current fb commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:49:43 | creating task(s): empathetic_dialogues\n",
      "18:49:44 | training...\n",
      "18:49:57 | time:12s total_exs:800 total_steps:50 epochs:0.01\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   29.93     1 444.7  1839  .09375      2.132 66.16  800  .9782 16.53 9.343   1 262.7  1086   .0050      .1050 11414   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "       .06044         0                   50 707.4 2925 4.136\n",
      "\n",
      "18:50:08 | time:24s total_exs:1600 total_steps:100 epochs:0.02\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   30.08     1 443.4  1958  .08125      2.369 70.64  800  1.006 16.07 8.962   1 254.5  1124   .0075      .1675 7803   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "       .09956         0                  100 697.9 3081 4.416\n",
      "\n",
      "18:50:20 | time:36s total_exs:2400 total_steps:150 epochs:0.04\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "    30.3     1 451.3  1925  .09625        2.1 68.24  800   1.06 16.29 8.724   1   260  1109  .00375     .04125 6149   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .1109         0                  150 711.3 3034 4.267\n",
      "\n",
      "18:50:30 | time:47s total_exs:3200 total_steps:200 epochs:0.05\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   30.02     1 452.3  2135   .0925      1.758 75.54  800  1.093 15.94 8.555   1   255  1204       0          0 5194   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .1216         0                  200 707.2 3339 4.723\n",
      "\n",
      "18:50:41 | time:57s total_exs:4000 total_steps:250 epochs:0.06\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   30.29     1 447.3  2153   .0950      2.326    77  800  1.118 16.48 8.386   1 261.8  1260   .0050      .1163 4386   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .1254         0                  250 709.1 3413 4.814\n",
      "\n",
      "18:50:52 | time:68s total_exs:4800 total_steps:300 epochs:0.07\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   31.48     1 460.1  2019   .0975      2.721 70.21  800  1.104  16.8 8.222   1 268.2  1177   .0075      .0450 3721   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .1359         0                  300 728.3 3196 4.389\n",
      "\n",
      "18:51:03 | time:80s total_exs:5600 total_steps:350 epochs:0.09\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   32.75     1 473.3  2147   .1150      3.167 72.58  800  1.129 17.29 8.171   1 275.8  1251  .00625      .0525 3536   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .1326         0                  350 749.1 3398 4.538\n",
      "\n",
      "18:51:14 | time:90s total_exs:6400 total_steps:400 epochs:0.10\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   30.34     1 449.9  2132  .09375      2.221 75.84  800  1.162 16.37 8.018   1 261.7  1240  .00125     .01375 3036   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .1463         0                  400 711.6 3373 4.741\n",
      "\n",
      "18:51:24 | time:100s total_exs:7200 total_steps:450 epochs:0.11\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   31.04     1 460.4  2280  .09625      2.261 79.24  800  1.195 15.87 7.937   1 253.7  1256  .00125     .01875 2800   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .1486         0                  450 714.1 3537 4.955\n",
      "\n",
      "18:51:35 | time:111s total_exs:8000 total_steps:500 epochs:0.12\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   29.39     1   436  2052  .07875      2.141 75.33  800  1.192 16.41 7.774   1 261.8  1232  .00625      .0450 2378   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
      "        .1619         0                  500 697.7 3285 4.71\n",
      "\n",
      "18:51:44 | max_train_time elapsed:120.04075765609741s\n",
      "18:51:44 | loading dictionary from from_scratch_model/model.dict\n",
      "18:51:44 | num words = 22419\n",
      "18:51:44 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
      "18:51:44 | Loading existing model params from from_scratch_model/model\n",
      "18:51:44 | creating task(s): empathetic_dialogues\n",
      "18:51:44 | running eval: valid\n",
      "18:53:37 | eval completed in 112.74s\n",
      "18:53:37 | \u001b[1mvalid:\n",
      "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  llen  loss  lr  ltpb  ltps  ltrunc  \\\n",
      "           0 5.168e-09 39.52 572.6  1829   .1682      3.592 50.91 5738 .1088       28.62 15.65 7.496   1 249.1 795.6 .002091   \n",
      "    ltrunclen  ppl  precision  recall  token_acc  token_em  total_train_updates   tpb  tps  \n",
      "       .02457 1801      .1609   .1041      .1750         0                  544 821.7 2624\n",
      "\u001b[0m\n",
      "18:53:37 | creating task(s): empathetic_dialogues\n",
      "18:53:38 | running eval: test\n",
      "18:55:23 | eval completed in 105.46s\n",
      "18:55:23 | \u001b[1mtest:\n",
      "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  llen  loss  lr  ltpb  ltps  ltrunc  \\\n",
      "           0 5.248e-09 42.71 604.5  1886   .1960      4.891 49.88 5259 .1053        30.5 15.85 7.508   1 252.6 788.1 .003613   \n",
      "    ltrunclen  ppl  precision  recall  token_acc  token_em  total_train_updates   tpb  tps  \n",
      "       .04982 1823      .1541   .1030      .1730         0                  544 857.1 2674\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'exs': SumMetric(5738),\n",
       "  'accuracy': ExactMatchMetric(0),\n",
       "  'precision': F1Metric(0.1609),\n",
       "  'recall': F1Metric(0.1041),\n",
       "  'f1': F1Metric(0.1088),\n",
       "  'bleu-4': BleuMetric(5.168e-09),\n",
       "  'clen': AverageMetric(39.52),\n",
       "  'ctrunc': AverageMetric(0.1682),\n",
       "  'ctrunclen': AverageMetric(3.592),\n",
       "  'llen': AverageMetric(15.65),\n",
       "  'ltrunc': AverageMetric(0.002091),\n",
       "  'ltrunclen': AverageMetric(0.02457),\n",
       "  'loss': AverageMetric(7.496),\n",
       "  'ppl': PPLMetric(1801),\n",
       "  'token_acc': AverageMetric(0.175),\n",
       "  'token_em': AverageMetric(0),\n",
       "  'gen_n_toks': AverageMetric(28.62),\n",
       "  'exps': GlobalTimerMetric(50.91),\n",
       "  'ltpb': GlobalAverageMetric(249.1),\n",
       "  'ltps': GlobalTimerMetric(795.6),\n",
       "  'ctpb': GlobalAverageMetric(572.6),\n",
       "  'ctps': GlobalTimerMetric(1829),\n",
       "  'tpb': GlobalAverageMetric(821.7),\n",
       "  'tps': GlobalTimerMetric(2624),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'total_train_updates': GlobalFixedMetric(544)},\n",
       " {'exs': SumMetric(5259),\n",
       "  'accuracy': ExactMatchMetric(0),\n",
       "  'precision': F1Metric(0.1541),\n",
       "  'recall': F1Metric(0.103),\n",
       "  'f1': F1Metric(0.1053),\n",
       "  'bleu-4': BleuMetric(5.248e-09),\n",
       "  'clen': AverageMetric(42.71),\n",
       "  'ctrunc': AverageMetric(0.196),\n",
       "  'ctrunclen': AverageMetric(4.891),\n",
       "  'llen': AverageMetric(15.85),\n",
       "  'ltrunc': AverageMetric(0.003613),\n",
       "  'ltrunclen': AverageMetric(0.04982),\n",
       "  'loss': AverageMetric(7.508),\n",
       "  'ppl': PPLMetric(1823),\n",
       "  'token_acc': AverageMetric(0.173),\n",
       "  'token_em': AverageMetric(0),\n",
       "  'gen_n_toks': AverageMetric(30.5),\n",
       "  'exps': GlobalTimerMetric(49.88),\n",
       "  'ltpb': GlobalAverageMetric(252.6),\n",
       "  'ltps': GlobalTimerMetric(788.1),\n",
       "  'ctpb': GlobalAverageMetric(604.5),\n",
       "  'ctps': GlobalTimerMetric(1886),\n",
       "  'tpb': GlobalAverageMetric(857.1),\n",
       "  'tps': GlobalTimerMetric(2674),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'total_train_updates': GlobalFixedMetric(544)})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll save it in the \"from_scratch_model\" directory\n",
    "!rm -rf from_scratch_model\n",
    "!mkdir -p from_scratch_model\n",
    "\n",
    "from parlai.scripts.train_model import TrainModel\n",
    "TrainModel.main(\n",
    "    # we MUST provide a filename\n",
    "    model_file='from_scratch_model/model',\n",
    "    # train on empathetic dialogues\n",
    "    task='empathetic_dialogues',\n",
    "    # limit training time to 2 minutes, and a batchsize of 16\n",
    "    max_train_time=2 * 60,\n",
    "    batchsize=16,\n",
    "    \n",
    "    # we specify the model type as seq2seq\n",
    "    model='seq2seq',\n",
    "    # some hyperparamter choices. We'll use attention. We could use pretrained\n",
    "    # embeddings too, with embedding_type='fasttext', but they take a long\n",
    "    # time to download.\n",
    "    attention='dot',\n",
    "    # tie the word embeddings of the encoder/decoder/softmax.\n",
    "    lookuptable='all',\n",
    "    # truncate text and labels at 64 tokens, for memory and time savings\n",
    "    truncate=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvA77Zwkoviq"
   },
   "source": [
    "Our perplexity and F1 (word overlap) scores are pretty bad, and our BLEU-4 score is nearly 0. That's okay, we would normally want to train for well over an hour. Feel free to change the max_train_time above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QTiTn7aoxv9"
   },
   "source": [
    "## Performance is pretty bad there. Can we improve it?\n",
    "\n",
    "The easiest way to improve it is to *initialize* using a *pretrained model*, utilizing *transfer learning*. Let's use the one from the interactive session at the beginning of the chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "O2Jt9bHTn1dP",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6082dcd0-3581-4526-f793-d032395cb6b9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:55:25 | building dictionary first...\n",
      "18:55:25 | No model with opt yet at: from_pretrained/model(.opt)\n",
      "18:55:25 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,is_debug: False,datapath: /home/jovyan/ParlAI/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,world_logs: ,save_format: conversations,seed: None,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,wandb_log_model: False,teacher_seed: None,mutators: None,train_experiencer_only: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,lambda_decay: 0.9,omega_bound: 0.3,p_reset: True,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,gpu_beam_blocking: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,parlai_home: /home/jovyan/ParlAI\u001b[0m\n",
      "18:55:25 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
      "18:55:25 | loading dictionary from /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model.dict\n",
      "18:55:25 | num words = 54944\n",
      "18:55:26 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "18:55:26 | Loading existing model params from /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model\n",
      "18:55:27 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
      "18:55:27 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
      "18:55:27 | Opt:\n",
      "18:55:27 |     activation: gelu\n",
      "18:55:27 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "18:55:27 |     adam_eps: 1e-08\n",
      "18:55:27 |     add_p1_after_newln: False\n",
      "18:55:27 |     aggregate_micro: False\n",
      "18:55:27 |     allow_missing_init_opts: False\n",
      "18:55:27 |     attention_dropout: 0.0\n",
      "18:55:27 |     batchsize: 12\n",
      "18:55:27 |     beam_block_full_context: True\n",
      "18:55:27 |     beam_block_list_filename: None\n",
      "18:55:27 |     beam_block_ngram: -1\n",
      "18:55:27 |     beam_context_block_ngram: -1\n",
      "18:55:27 |     beam_delay: 30\n",
      "18:55:27 |     beam_length_penalty: 0.65\n",
      "18:55:27 |     beam_min_length: 1\n",
      "18:55:27 |     beam_size: 1\n",
      "18:55:27 |     betas: '(0.9, 0.999)'\n",
      "18:55:27 |     bpe_add_prefix_space: None\n",
      "18:55:27 |     bpe_debug: False\n",
      "18:55:27 |     bpe_dropout: None\n",
      "18:55:27 |     bpe_merge: None\n",
      "18:55:27 |     bpe_vocab: None\n",
      "18:55:27 |     checkpoint_activations: False\n",
      "18:55:27 |     compute_tokenized_bleu: False\n",
      "18:55:27 |     datapath: /home/jovyan/ParlAI/data\n",
      "18:55:27 |     datatype: train\n",
      "18:55:27 |     delimiter: '\\n'\n",
      "18:55:27 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "18:55:27 |     dict_endtoken: __end__\n",
      "18:55:27 |     dict_file: /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model.dict\n",
      "18:55:27 |     dict_include_test: False\n",
      "18:55:27 |     dict_include_valid: False\n",
      "18:55:27 |     dict_initpath: None\n",
      "18:55:27 |     dict_language: english\n",
      "18:55:27 |     dict_loaded: True\n",
      "18:55:27 |     dict_lower: True\n",
      "18:55:27 |     dict_max_ngram_size: -1\n",
      "18:55:27 |     dict_maxexs: -1\n",
      "18:55:27 |     dict_maxtokens: -1\n",
      "18:55:27 |     dict_minfreq: 0\n",
      "18:55:27 |     dict_nulltoken: __null__\n",
      "18:55:27 |     dict_starttoken: __start__\n",
      "18:55:27 |     dict_textfields: text,labels\n",
      "18:55:27 |     dict_tokenizer: bpe\n",
      "18:55:27 |     dict_unktoken: __unk__\n",
      "18:55:27 |     display_examples: False\n",
      "18:55:27 |     download_path: None\n",
      "18:55:27 |     dropout: 0.0\n",
      "18:55:27 |     dynamic_batching: full\n",
      "18:55:27 |     embedding_projection: random\n",
      "18:55:27 |     embedding_size: 512\n",
      "18:55:27 |     embedding_type: random\n",
      "18:55:27 |     embeddings_scale: True\n",
      "18:55:27 |     eval_batchsize: None\n",
      "18:55:27 |     eval_dynamic_batching: None\n",
      "18:55:27 |     evaltask: None\n",
      "18:55:27 |     ffn_size: 2048\n",
      "18:55:27 |     final_extra_opt: \n",
      "18:55:27 |     force_fp16_tokens: False\n",
      "18:55:27 |     fp16: True\n",
      "18:55:27 |     fp16_impl: mem_efficient\n",
      "18:55:27 |     gpu: -1\n",
      "18:55:27 |     gpu_beam_blocking: False\n",
      "18:55:27 |     gradient_clip: 0.1\n",
      "18:55:27 |     hide_labels: False\n",
      "18:55:27 |     history_add_global_end_token: None\n",
      "18:55:27 |     history_reversed: False\n",
      "18:55:27 |     history_size: -1\n",
      "18:55:27 |     image_cropsize: 224\n",
      "18:55:27 |     image_mode: raw\n",
      "18:55:27 |     image_size: 256\n",
      "18:55:27 |     inference: greedy\n",
      "18:55:27 |     init_model: /home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model\n",
      "18:55:27 |     init_opt: None\n",
      "18:55:27 |     interactive_mode: False\n",
      "18:55:27 |     invsqrt_lr_decay_gamma: -1\n",
      "18:55:27 |     is_debug: False\n",
      "18:55:27 |     label_truncate: 128\n",
      "18:55:27 |     lambda_decay: 0.9\n",
      "18:55:27 |     learn_positional_embeddings: True\n",
      "18:55:27 |     learningrate: 1e-05\n",
      "18:55:27 |     load_from_checkpoint: True\n",
      "18:55:27 |     log_every_n_secs: -1\n",
      "18:55:27 |     log_every_n_steps: 50\n",
      "18:55:27 |     log_keep_fields: all\n",
      "18:55:27 |     loglevel: info\n",
      "18:55:27 |     lr_scheduler: reduceonplateau\n",
      "18:55:27 |     lr_scheduler_decay: 0.5\n",
      "18:55:27 |     lr_scheduler_patience: 3\n",
      "18:55:27 |     max_train_steps: -1\n",
      "18:55:27 |     max_train_time: 600.0\n",
      "18:55:27 |     metrics: default\n",
      "18:55:27 |     model: transformer/generator\n",
      "18:55:27 |     model_file: from_pretrained/model\n",
      "18:55:27 |     model_parallel: False\n",
      "18:55:27 |     momentum: 0\n",
      "18:55:27 |     multitask_weights: [1]\n",
      "18:55:27 |     mutators: None\n",
      "18:55:27 |     n_decoder_layers: -1\n",
      "18:55:27 |     n_encoder_layers: -1\n",
      "18:55:27 |     n_heads: 16\n",
      "18:55:27 |     n_layers: 8\n",
      "18:55:27 |     n_positions: 512\n",
      "18:55:27 |     n_segments: 0\n",
      "18:55:27 |     nesterov: True\n",
      "18:55:27 |     no_cuda: False\n",
      "18:55:27 |     num_epochs: -1\n",
      "18:55:27 |     num_workers: 0\n",
      "18:55:27 |     nus: (0.7,)\n",
      "18:55:27 |     omega_bound: 0.3\n",
      "18:55:27 |     optimizer: adam\n",
      "18:55:27 |     output_scaling: 1.0\n",
      "18:55:27 |     override: \"{'task': 'empathetic_dialogues', 'model': 'transformer/generator', 'model_file': 'from_pretrained/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/home/jovyan/ParlAI/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 600.0, 'validation_every_n_epochs': 0.25, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
      "18:55:27 |     p_reset: True\n",
      "18:55:27 |     parlai_home: /home/jovyan/ParlAI\n",
      "18:55:27 |     person_tokens: False\n",
      "18:55:27 |     rank_candidates: False\n",
      "18:55:27 |     relu_dropout: 0.0\n",
      "18:55:27 |     save_after_valid: False\n",
      "18:55:27 |     save_every_n_secs: -1\n",
      "18:55:27 |     save_format: conversations\n",
      "18:55:27 |     seed: None\n",
      "18:55:27 |     share_word_embeddings: True\n",
      "18:55:27 |     short_final_eval: False\n",
      "18:55:27 |     skip_generation: True\n",
      "18:55:27 |     special_tok_lst: None\n",
      "18:55:27 |     split_lines: False\n",
      "18:55:27 |     starttime: Jan26_18-55\n",
      "18:55:27 |     task: empathetic_dialogues\n",
      "18:55:27 |     teacher_seed: None\n",
      "18:55:27 |     temperature: 1.0\n",
      "18:55:27 |     tensorboard_log: False\n",
      "18:55:27 |     tensorboard_logdir: None\n",
      "18:55:27 |     text_truncate: 512\n",
      "18:55:27 |     topk: 10\n",
      "18:55:27 |     topp: 0.9\n",
      "18:55:27 |     train_experiencer_only: False\n",
      "18:55:27 |     truncate: -1\n",
      "18:55:27 |     update_freq: 1\n",
      "18:55:27 |     use_reply: label\n",
      "18:55:27 |     validation_cutoff: 1.0\n",
      "18:55:27 |     validation_every_n_epochs: 0.25\n",
      "18:55:27 |     validation_every_n_secs: -1\n",
      "18:55:27 |     validation_every_n_steps: -1\n",
      "18:55:27 |     validation_max_exs: -1\n",
      "18:55:27 |     validation_metric: ppl\n",
      "18:55:27 |     validation_metric_mode: None\n",
      "18:55:27 |     validation_patience: 10\n",
      "18:55:27 |     validation_share_agent: False\n",
      "18:55:27 |     variant: xlm\n",
      "18:55:27 |     verbose: False\n",
      "18:55:27 |     wandb_entity: None\n",
      "18:55:27 |     wandb_log: False\n",
      "18:55:27 |     wandb_log_model: False\n",
      "18:55:27 |     wandb_name: None\n",
      "18:55:27 |     wandb_project: None\n",
      "18:55:27 |     warmup_rate: 0.0001\n",
      "18:55:27 |     warmup_updates: 100\n",
      "18:55:27 |     weight_decay: None\n",
      "18:55:27 |     world_logs: \n",
      "18:55:27 | Current ParlAI commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:55:27 | Current internal commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:55:27 | Current fb commit: cb369e94ffb4e6976c2c6009fcb9353192d33c37\n",
      "18:55:27 | creating task(s): empathetic_dialogues\n",
      "18:55:32 | training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrm -rf from_pretrained\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir -p from_pretrained\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mTrainModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# similar to before\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mempathetic_dialogues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer/generator\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfrom_pretrained/model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# initialize with a pretrained model\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzoo:tutorial_transformer_generator/model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# arguments we get from the pretrained model.\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Unfortunately, these must be looked up separately for each model.\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_truncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_truncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mffn_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxlm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_lower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbpe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzoo:tutorial_transformer_generator/model.dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearn_positional_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# some training arguments, specific to this fine-tuning\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use a small learning rate with ADAM optimizer\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# early stopping on perplexity\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mppl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train at most 10 minutes, and validate every 0.25 epochs\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_train_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_every_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# depend on your gpu. If you have a V100, this is good\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16_impl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmem_efficient\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# speeds up validation\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# helps us cram more examples into our gpu at a time\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_batching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ParlAI/parlai/core/script.py:127\u001b[0m, in \u001b[0;36mParlaiScript.main\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_args(args)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kwargs:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_args(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/ParlAI/parlai/core/script.py:92\u001b[0m, in \u001b[0;36mParlaiScript._run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_args()\n\u001b[1;32m     91\u001b[0m opt \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_from_parser_and_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ParlAI/parlai/core/script.py:108\u001b[0m, in \u001b[0;36mParlaiScript._run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(opt)\n\u001b[1;32m    107\u001b[0m script\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m parser\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscript\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ParlAI/parlai/scripts/train_model.py:1060\u001b[0m, in \u001b[0;36mTrainModel.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     set_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m-> 1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ParlAI/parlai/scripts/train_model.py:1010\u001b[0m, in \u001b[0;36mTrainLoop.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03mPerform a training run.\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m:return: tuple of reports (validation_report, test_report)\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\n\u001b[0;32m-> 1010\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _train_log \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_steps():\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# we've already done what we need in these\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# perform final validation/testing\u001b[39;00m\n",
      "File \u001b[0;32m~/ParlAI/parlai/scripts/train_model.py:917\u001b[0m, in \u001b[0;36mTrainLoop.train_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;66;03m# do one example / batch of examples\u001b[39;00m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         \u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparley\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m StopTrainException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    919\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopping from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ParlAI/parlai/core/worlds.py:1212\u001b[0m, in \u001b[0;36mDynamicBatchWorld.parley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batch_size\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# great, this batch is good to go! let's run it!\u001b[39;00m\n\u001b[0;32m-> 1212\u001b[0m acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_acts[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch], acts]\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# broadcast the results back to all the models\u001b[39;00m\n",
      "File \u001b[0;32m~/ParlAI/parlai/core/worlds.py:1236\u001b[0m, in \u001b[0;36mDynamicBatchWorld.handle_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m-> 1236\u001b[0m     acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acts\n",
      "File \u001b[0;32m~/ParlAI/parlai/core/torch_agent.py:2248\u001b[0m, in \u001b[0;36mTorchAgent.batch_act\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_training:\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;66;03m# register the start of updates for later counting when they occur\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_metrics\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mups\u001b[39m\u001b[38;5;124m'\u001b[39m, GlobalTimerMetric(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m-> 2248\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;66;03m# save memory and compute by disabling autograd.\u001b[39;00m\n\u001b[1;32m   2252\u001b[0m         \u001b[38;5;66;03m# use `with torch.enable_grad()` to gain back gradients.\u001b[39;00m\n",
      "File \u001b[0;32m~/ParlAI/parlai/core/torch_generator_agent.py:784\u001b[0m, in \u001b[0;36mTorchGeneratorAgent.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(batch)\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params()\n\u001b[1;32m    786\u001b[0m     oom_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/ParlAI/parlai/core/torch_agent.py:2336\u001b[0m, in \u001b[0;36mTorchAgent.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mbackward(loss, update_main_grads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2336\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parlai_env/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parlai_env/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!rm -rf from_pretrained\n",
    "!mkdir -p from_pretrained\n",
    "\n",
    "TrainModel.main(\n",
    "    # similar to before\n",
    "    task='empathetic_dialogues', \n",
    "    model='transformer/generator',\n",
    "    model_file='from_pretrained/model',\n",
    "    \n",
    "    # initialize with a pretrained model\n",
    "    init_model='zoo:tutorial_transformer_generator/model',\n",
    "    \n",
    "    # arguments we get from the pretrained model.\n",
    "    # Unfortunately, these must be looked up separately for each model.\n",
    "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
    "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
    "    activation='gelu', variant='xlm',\n",
    "    dict_lower=True, dict_tokenizer='bpe',\n",
    "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
    "    learn_positional_embeddings=True,\n",
    "    \n",
    "    # some training arguments, specific to this fine-tuning\n",
    "    # use a small learning rate with ADAM optimizer\n",
    "    lr=1e-5, optimizer='adam',\n",
    "    warmup_updates=100,\n",
    "    # early stopping on perplexity\n",
    "    validation_metric='ppl',\n",
    "    # train at most 10 minutes, and validate every 0.25 epochs\n",
    "    max_train_time=600, validation_every_n_epochs=0.25,\n",
    "    \n",
    "    # depend on your gpu. If you have a V100, this is good\n",
    "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
    "    \n",
    "    # speeds up validation\n",
    "    skip_generation=True,\n",
    "    \n",
    "    # helps us cram more examples into our gpu at a time\n",
    "    dynamic_batching='full',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iBZXTLRvIjb"
   },
   "source": [
    "## Wow that's a lot of options? Where do I find more info?\n",
    "\n",
    "As you might have noticed, there are a LOT of options to ParlAI. You're best reading the [ParlAI docs](https://parl.ai/docs) to find a list of hyperparameters. We provide lists of the command-line args for both models\n",
    "\n",
    "You can get some guidance in this notebook by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "0Pl8VVl5plfm",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "453c1f82-a7ff-4df7-c7ad-f728c6e67854",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: TrainModel [-h] [-o INIT_OPT] [-v] [-t TASK]\n",
      "                  [-dt {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}]\n",
      "                  [-nt NUMTHREADS] [-bs BATCHSIZE] [-dynb {None,batchsort,full}]\n",
      "                  [-dp DATAPATH] [-m MODEL] [-mf MODEL_FILE] [-im INIT_MODEL]\n",
      "                  [-et EVALTASK] [-eps NUM_EPOCHS] [-ttim MAX_TRAIN_TIME]\n",
      "                  [-vtim VALIDATION_EVERY_N_SECS] [-stim SAVE_EVERY_N_SECS]\n",
      "                  [-sval SAVE_AFTER_VALID] [-veps VALIDATION_EVERY_N_EPOCHS]\n",
      "                  [-vp VALIDATION_PATIENCE] [-vmt VALIDATION_METRIC]\n",
      "                  [-vmm {max,min}] [-mcs METRICS] [-micro AGGREGATE_MICRO]\n",
      "                  [-tblog TENSORBOARD_LOG] [-hs HIDDENSIZE] [-esz EMBEDDINGSIZE]\n",
      "                  [-nl NUMLAYERS] [-dr DROPOUT] [-bi BIDIRECTIONAL]\n",
      "                  [-att {none,concat,general,dot,local}]\n",
      "                  [-attl ATTENTION_LENGTH] [--attention-time {pre,post}]\n",
      "                  [-rnn {rnn,gru,lstm}] [-dec {same,shared}]\n",
      "                  [-lt {unique,enc_dec,dec_out,all}] [-soft NUMSOFTMAX]\n",
      "                  [-idr INPUT_DROPOUT] [--beam-size BEAM_SIZE]\n",
      "                  [--beam-min-length BEAM_MIN_LENGTH]\n",
      "                  [--beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM]\n",
      "                  [--beam-block-ngram BEAM_BLOCK_NGRAM]\n",
      "                  [--beam-length-penalty BEAM_LENGTH_PENALTY]\n",
      "                  [--inference {topk,beam,nucleus,delayedbeam,greedy}]\n",
      "                  [--topk TOPK] [--topp TOPP] [--beam-delay BEAM_DELAY]\n",
      "                  [--temperature TEMPERATURE]\n",
      "                  [--compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU]\n",
      "                  [-i INTERACTIVE_MODE]\n",
      "                  [-emb {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}]\n",
      "                  [-embp EMBEDDING_PROJECTION] [--fp16 FP16]\n",
      "                  [--fp16-impl {apex,mem_efficient}]\n",
      "                  [-opt {adadelta,adagrad,adam,adamw,sparseadam,adamax,asgd,sgd,rprop,rmsprop,optimizer,lbfgs,mem_eff_adam,adafactor}]\n",
      "                  [-lr LEARNINGRATE] [-clip GRADIENT_CLIP]\n",
      "                  [--adafactor-eps ADAFACTOR_EPS] [-mom MOMENTUM]\n",
      "                  [--nesterov NESTEROV] [-nu NUS] [-beta BETAS]\n",
      "                  [-wdecay WEIGHT_DECAY] [-rc RANK_CANDIDATES] [-tr TRUNCATE]\n",
      "                  [--text-truncate TEXT_TRUNCATE]\n",
      "                  [--label-truncate LABEL_TRUNCATE] [-histsz HISTORY_SIZE]\n",
      "                  [-pt PERSON_TOKENS] [--split-lines SPLIT_LINES]\n",
      "                  [--delimiter DELIMITER] [-gpu GPU | --no-cuda]\n",
      "                  [--bpe-vocab BPE_VOCAB] [--bpe-merge BPE_MERGE]\n",
      "                  [--lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}]\n",
      "                  [--lr-scheduler-patience LR_SCHEDULER_PATIENCE]\n",
      "                  [--lr-scheduler-decay LR_SCHEDULER_DECAY]\n",
      "                  [--max-lr-steps MAX_LR_STEPS]\n",
      "                  [--invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA]\n",
      "\n",
      "Train a model\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help\n",
      "      show this help message and exit\n",
      "\n",
      "Main ParlAI Arguments:\n",
      "  -o, --init-opt INIT_OPT\n",
      "      Path to json file of options. Note: Further Command-line arguments\n",
      "      override file-based options. (default: None)\n",
      "  -v, --show-advanced-args\n",
      "      Show hidden command line options (advanced users only) (default: False)\n",
      "  -t, --task TASK\n",
      "      ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
      "  -dt, --datatype {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}\n",
      "      choose from: train, train:ordered, valid, test. to stream data add\n",
      "      \":stream\" to any option (e.g., train:stream). by default: train is random\n",
      "      with replacement, valid is ordered, test is ordered. (default: train)\n",
      "  -nt, --numthreads NUMTHREADS\n",
      "      number of threads. Used for hogwild if batchsize is 1, else for number of\n",
      "      threads in threadpool loading, (default: 1)\n",
      "  -bs, --batchsize BATCHSIZE\n",
      "      batch size for minibatch training schemes (default: 1)\n",
      "  -dynb, --dynamic-batching {None,batchsort,full}\n",
      "      Use dynamic batching (default: None)\n",
      "  -dp, --datapath DATAPATH\n",
      "      path to datasets, defaults to {parlai_dir}/data (default: None)\n",
      "\n",
      "ParlAI Model Arguments:\n",
      "  -m, --model MODEL\n",
      "      the model class name. can match parlai/agents/<model> for agents in that\n",
      "      directory, or can provide a fully specified module for `from X import Y`\n",
      "      via `-m X:Y` (e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)\n",
      "      (default: None)\n",
      "  -mf, --model-file MODEL_FILE\n",
      "      model file name for loading and saving models (default: None)\n",
      "  -im, --init-model INIT_MODEL\n",
      "      load model weights and dict from this file (default: None)\n",
      "\n",
      "Training Loop Arguments:\n",
      "  -et, --evaltask EVALTASK\n",
      "      task to use for valid/test (defaults to the one used for training)\n",
      "      (default: None)\n",
      "  -eps, --num-epochs NUM_EPOCHS\n",
      "  -ttim, --max-train-time MAX_TRAIN_TIME\n",
      "  -vtim, --validation-every-n-secs VALIDATION_EVERY_N_SECS\n",
      "      Validate every n seconds. Saves model to model_file (if set) whenever best\n",
      "      val metric is found (default: -1)\n",
      "  -stim, --save-every-n-secs SAVE_EVERY_N_SECS\n",
      "      Saves the model to model_file.checkpoint after every n seconds (default\n",
      "      -1, never). (default: -1)\n",
      "  -sval, --save-after-valid SAVE_AFTER_VALID\n",
      "      Saves the model to model_file.checkpoint after every validation (default\n",
      "      False).\n",
      "  -veps, --validation-every-n-epochs VALIDATION_EVERY_N_EPOCHS\n",
      "      Validate every n epochs. Saves model to model_file (if set) whenever best\n",
      "      val metric is found (default: -1)\n",
      "  -vp, --validation-patience VALIDATION_PATIENCE\n",
      "      number of iterations of validation where result does not improve before we\n",
      "      stop training (default: 10)\n",
      "  -vmt, --validation-metric VALIDATION_METRIC\n",
      "      key into report table for selecting best validation (default: accuracy)\n",
      "  -vmm, --validation-metric-mode {max,min}\n",
      "      how to optimize validation metric (max or min) (default: None)\n",
      "  -mcs, --metrics METRICS\n",
      "      list of metrics to show/compute, e.g. all, default,or give a list split by\n",
      "      , like ppl,f1,accuracy,hits@1,rouge,bleuthe rouge metrics will be computed\n",
      "      as rouge-1, rouge-2 and rouge-l (default: default)\n",
      "  -micro, --aggregate-micro AGGREGATE_MICRO\n",
      "      Report micro-averaged metrics instead of macro averaged metrics. (default:\n",
      "      False)\n",
      "\n",
      "Tensorboard Arguments:\n",
      "  -tblog, --tensorboard-log TENSORBOARD_LOG\n",
      "      Tensorboard logging of metrics, default is False\n",
      "\n",
      "Seq2Seq Arguments:\n",
      "  -hs, --hiddensize HIDDENSIZE\n",
      "      size of the hidden layers (default: 128)\n",
      "  -esz, --embeddingsize EMBEDDINGSIZE\n",
      "      size of the token embeddings (default: 128)\n",
      "  -nl, --numlayers NUMLAYERS\n",
      "      number of hidden layers (default: 2)\n",
      "  -dr, --dropout DROPOUT\n",
      "      dropout rate (default: 0.1)\n",
      "  -bi, --bidirectional BIDIRECTIONAL\n",
      "      whether to encode the context with a bidirectional rnn (default: False)\n",
      "  -att, --attention {none,concat,general,dot,local}\n",
      "      Choices: none, concat, general, local. If set local, also set attention-\n",
      "      length. (see arxiv.org/abs/1508.04025) (default: none)\n",
      "  -attl, --attention-length ATTENTION_LENGTH\n",
      "      Length of local attention. (default: 48)\n",
      "  --attention-time {pre,post}\n",
      "      Whether to apply attention before or after decoding. (default: post)\n",
      "  -rnn, --rnn-class {rnn,gru,lstm}\n",
      "      Choose between different types of RNNs. (default: lstm)\n",
      "  -dec, --decoder {same,shared}\n",
      "      Choose between different decoder modules. Default \"same\" uses same class\n",
      "      as encoder, while \"shared\" also uses the same weights. Note that shared\n",
      "      disabled some encoder options--in particular, bidirectionality. (default:\n",
      "      same)\n",
      "  -lt, --lookuptable {unique,enc_dec,dec_out,all}\n",
      "      The encoder, decoder, and output modules can share weights, or not. Unique\n",
      "      has independent embeddings for each. Enc_dec shares the embedding for the\n",
      "      encoder and decoder. Dec_out shares decoder embedding and output weights.\n",
      "      All shares all three weights. (default: unique)\n",
      "  -soft, --numsoftmax NUMSOFTMAX\n",
      "      default 1, if greater then uses mixture of softmax (see\n",
      "      arxiv.org/abs/1711.03953). (default: 1)\n",
      "  -idr, --input-dropout INPUT_DROPOUT\n",
      "      Probability of replacing tokens with UNK in training. (default: 0.0)\n",
      "\n",
      "Torch Generator Agent:\n",
      "  --beam-size BEAM_SIZE\n",
      "      Beam size, if 1 then greedy search (default: 1)\n",
      "  --beam-min-length BEAM_MIN_LENGTH\n",
      "      Minimum length of prediction to be generated by the beam search (default:\n",
      "      1)\n",
      "  --beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM\n",
      "      Size n-grams to block in beam search from the context. val <= 0 implies no\n",
      "      blocking (default: -1)\n",
      "  --beam-block-ngram BEAM_BLOCK_NGRAM\n",
      "      Size n-grams to block in beam search. val <= 0 implies no blocking\n",
      "      (default: -1)\n",
      "  --beam-length-penalty BEAM_LENGTH_PENALTY\n",
      "      Applies a length penalty. Set to 0 for no penalty. (default: 0.65)\n",
      "  --inference {topk,beam,nucleus,delayedbeam,greedy}\n",
      "      Generation algorithm (default: greedy)\n",
      "  --topk TOPK\n",
      "      K used in Top K sampling (default: 10)\n",
      "  --topp TOPP\n",
      "      p used in nucleus sampling (default: 0.9)\n",
      "  --beam-delay BEAM_DELAY\n",
      "      used in delayedbeam search (default: 30)\n",
      "  --temperature TEMPERATURE\n",
      "      temperature to add during decoding (default: 1.0)\n",
      "  --compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU\n",
      "      if true, compute tokenized bleu scores (default: False)\n",
      "\n",
      "TorchAgent Arguments:\n",
      "  -i, --interactive-mode INTERACTIVE_MODE\n",
      "      Whether in full interactive mode or not, which means generating text or\n",
      "      retrieving from a full set of candidates, which is necessary to actually\n",
      "      do full dialogue. However, during training or quick validation (e.g. PPL\n",
      "      for generation or ranking a few candidates for ranking models) you might\n",
      "      want these set to off. Typically, scripts can set their preferred default\n",
      "      behavior at the start, e.g. eval scripts. (default: False)\n",
      "  -emb, --embedding-type {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}\n",
      "      Choose between different strategies for initializing word embeddings.\n",
      "      Default is random, but can also preinitialize from Glove or Fasttext.\n",
      "      Preinitialized embeddings can also be fixed so they are not updated during\n",
      "      training. (default: random)\n",
      "  -embp, --embedding-projection EMBEDDING_PROJECTION\n",
      "      If pretrained embeddings have a different dimensionality than your\n",
      "      embedding size, strategy for projecting to the correct size. If the\n",
      "      dimensions are the same, this is ignored unless you append \"-force\" to\n",
      "      your choice. (default: random)\n",
      "  --fp16 FP16\n",
      "      Use fp16 computations. (default: False)\n",
      "  --fp16-impl {apex,mem_efficient}\n",
      "      Implementation of FP16 to use (default: apex)\n",
      "  -rc, --rank-candidates RANK_CANDIDATES\n",
      "      Whether the model should parse candidates for ranking. (default: False)\n",
      "  -tr, --truncate TRUNCATE\n",
      "      Truncate input lengths to increase speed / use less memory. (default: -1)\n",
      "  --text-truncate TEXT_TRUNCATE\n",
      "      Text input truncation length: if not specified, this will default to\n",
      "      `truncate` (default: None)\n",
      "  --label-truncate LABEL_TRUNCATE\n",
      "      Label truncation length: if not specified, this will default to `truncate`\n",
      "      (default: None)\n",
      "  -histsz, --history-size HISTORY_SIZE\n",
      "      Number of past dialog utterances to remember. (default: -1)\n",
      "  -pt, --person-tokens PERSON_TOKENS\n",
      "      add person tokens to history. adds __p1__ in front of input text and\n",
      "      __p2__ in front of past labels when available or past utterances generated\n",
      "      by the model. these are added to the dictionary during initialization.\n",
      "      (default: False)\n",
      "  --split-lines SPLIT_LINES\n",
      "      split the dialogue history on newlines and save in separate vectors\n",
      "      (default: False)\n",
      "  --delimiter DELIMITER\n",
      "      Join history lines with this token, defaults to newline (default: )\n",
      "  -gpu, --gpu GPU\n",
      "      which GPU to use (default: -1)\n",
      "  --no-cuda\n",
      "      disable GPUs even if available. otherwise, will use GPUs if available on\n",
      "      the device. (default: False)\n",
      "\n",
      "Optimizer Arguments:\n",
      "  -opt, --optimizer {adadelta,adagrad,adam,adamw,sparseadam,adamax,asgd,sgd,rprop,rmsprop,optimizer,lbfgs,mem_eff_adam,adafactor}\n",
      "      Choose between pytorch optimizers. Any member of torch.optim should be\n",
      "      valid. (default: sgd)\n",
      "  -lr, --learningrate LEARNINGRATE\n",
      "      Learning rate (default: 1)\n",
      "  -clip, --gradient-clip GRADIENT_CLIP\n",
      "      gradient clipping using l2 norm (default: 0.1)\n",
      "  --adafactor-eps ADAFACTOR_EPS\n",
      "      Epsilon values for adafactor optimizer: regularization constants for\n",
      "      square gradient and parameter scale respectively (default: 1e-30,1e-3)\n",
      "  -mom, --momentum MOMENTUM\n",
      "      if applicable, momentum value for optimizer. (default: 0)\n",
      "  --nesterov NESTEROV\n",
      "      if applicable, whether to use nesterov momentum. (default: True)\n",
      "  -nu, --nus NUS\n",
      "      if applicable, nu value(s) for optimizer. can use a single value like 0.7\n",
      "      or a comma-separated tuple like 0.7,1.0 (default: 0.7)\n",
      "  -beta, --betas BETAS\n",
      "      if applicable, beta value(s) for optimizer. can use a single value like\n",
      "      0.9 or a comma-separated tuple like 0.9,0.999 (default: 0.9,0.999)\n",
      "  -wdecay, --weight-decay WEIGHT_DECAY\n",
      "      Weight decay on the weights. (default: None)\n",
      "\n",
      "BPEHelper Arguments:\n",
      "  --bpe-vocab BPE_VOCAB\n",
      "      path to pre-trained tokenizer vocab (default: None)\n",
      "  --bpe-merge BPE_MERGE\n",
      "      path to pre-trained tokenizer merge (default: None)\n",
      "\n",
      "Learning Rate Scheduler:\n",
      "  --lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}\n",
      "      Learning rate scheduler. (default: reduceonplateau)\n",
      "  --lr-scheduler-patience LR_SCHEDULER_PATIENCE\n",
      "      LR scheduler patience. In number of validation runs. If using fixed\n",
      "      scheduler, LR is decayed every <patience> validations. (default: 3)\n",
      "  --lr-scheduler-decay LR_SCHEDULER_DECAY\n",
      "      Decay factor for LR scheduler, or how much LR is multiplied by when it is\n",
      "      lowered. (default: 0.5)\n",
      "  --max-lr-steps MAX_LR_STEPS\n",
      "      Number of train steps the scheduler should take after warmup. Training is\n",
      "      terminated after this many steps. This should only be set for --lr-\n",
      "      scheduler cosine or linear (default: -1)\n",
      "  --invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA\n",
      "      Constant used only to find the lr multiplier for the invsqrt scheduler.\n",
      "      Must be set for --lr-scheduler invsqrt (default: -1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note that if you want to see model-specific arguments, you must specify a model name\n",
    "print(TrainModel.help(model='seq2seq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKGUWyKTwVtX"
   },
   "source": [
    "You'll notice the options are give as commandline arguments. We control our options via `argparse`. The option names are relatively predictable: `--init-model` becomes `init_model`; `--num-epochs` becomes `num_epochs` and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgLwGAq1wZJb"
   },
   "source": [
    "# Looking at model predictions\n",
    "\n",
    "We have shown how we can chat with a model ourselves, interactively. We might want to inspect how the model reacts with a fixed set of inputs. Let's use that model we just trained!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "UCZgs6OlvJ-q",
    "outputId": "95b89fa8-22a6-4261-d72e-e82b07517810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ warning: overriding opt['num_examples'] to 2 (previously: None )]\n",
      "[ Using CUDA ]\n",
      "Dictionary: loading dictionary from from_pretrained/model.dict\n",
      "[ num words =  54944 ]\n",
      "Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "[ Loading existing model params from from_pretrained/model ]\n",
      "[creating task(s): empathetic_dialogues]\n",
      "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
      "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
      "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
      "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
      "\u001b[0;95m     model: No response\u001b[0;0m\n",
      "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
      "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
      "\u001b[0;95m     model: No response\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "from parlai.scripts.display_model import DisplayModel\n",
    "DisplayModel.main(\n",
    "    task='empathetic_dialogues',\n",
    "    model_file='from_pretrained/model',\n",
    "    num_examples=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H3QKTjdwokh"
   },
   "source": [
    "Whoa wait a second! The model isn't giving any responses? That's because we set `--skip-generation true` to speed up training. We need to turn that back off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "DLiq-vuowamh",
    "outputId": "241342eb-6ac6-4a0c-91c5-674ab6b7e0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ warning: overriding opt['num_examples'] to 2 (previously: None )]\n",
      "[ warning: overriding opt['skip_generation'] to False (previously: True )]\n",
      "[ Using CUDA ]\n",
      "Dictionary: loading dictionary from from_pretrained/model.dict\n",
      "[ num words =  54944 ]\n",
      "Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "[ Loading existing model params from from_pretrained/model ]\n",
      "[creating task(s): empathetic_dialogues]\n",
      "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
      "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
      "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
      "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
      "\u001b[0;95m     model: oh no ! that ' s terrible ! did you get a new tire ?\u001b[0;0m\n",
      "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
      "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
      "\u001b[0;95m     model: that ' s good . i hope you are okay .\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "from parlai.scripts.display_model import DisplayModel\n",
    "DisplayModel.main(\n",
    "    task='empathetic_dialogues',\n",
    "    model_file='from_pretrained/model',\n",
    "    num_examples=2,\n",
    "    skip_generation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MR0rn0ZwyxQ"
   },
   "source": [
    "On the command line:\n",
    "```bash\n",
    "python -m parlai.scripts.display_model --task empathetic_dialogues --model-file zoo:tutorial_transformer_generator/model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYuaSPWrw0Il"
   },
   "source": [
    "# Bringing your own datasets\n",
    "\n",
    "What if you want to build your own dataset in ParlAI? Of course you can do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "5SgJi8XHwtph",
    "outputId": "9656b678-9dfc-4cfa-d1eb-3cb13aa14608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from train.txt ~~ \n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
      "\u001b[0mHello\u001b[0;0m\n",
      "   \u001b[1;94mHi\u001b[0;0m\n",
      "\u001b[0mHow are you\u001b[0;0m\n",
      "   \u001b[1;94mI am fine\u001b[0;0m\n",
      "\u001b[0mLet's say goodbye\u001b[0;0m\n",
      "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
      "\u001b[0mHey\u001b[0;0m\n",
      "   \u001b[1;94mhi there\u001b[0;0m\n",
      "\u001b[0mDeja vu?\u001b[0;0m\n",
      "   \u001b[1;94mDeja vu!\u001b[0;0m\n",
      "\u001b[0mLast chance\u001b[0;0m\n",
      "   \u001b[1;94mThis is it\u001b[0;0m\n",
      "EPOCH DONE\n",
      "[ loaded 2 episodes with a total of 6 examples ]\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.teachers import register_teacher, DialogTeacher\n",
    "\n",
    "@register_teacher(\"my_teacher\")\n",
    "class MyTeacher(DialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        # opt is the command line arguments.\n",
    "        \n",
    "        # What is this shared thing?\n",
    "        # We make many copies of a teacher, one-per-batchsize. Shared lets us store \n",
    "        \n",
    "        # We just need to set the \"datafile\".  This is boilerplate, but differs in many teachers.\n",
    "        # The \"datafile\" is the filename where we will load the data from. In this case, we'll set it to\n",
    "        # the fold name (train/valid/test) + \".txt\"\n",
    "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
    "        super().__init__(opt, shared)\n",
    "    \n",
    "    def setup_data(self, datafile):\n",
    "        # filename tells us where to load from.\n",
    "        # We'll just use some hardcoded data, but show how you could read the filename here:\n",
    "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
    "        \n",
    "        # setup_data should yield tuples of ((text, label), new_episode)\n",
    "        # That is ((str, str), bool)\n",
    "        \n",
    "        # first episode\n",
    "        # notice how we have call, response, and then True? The True indicates this is a first message\n",
    "        # in a conversation\n",
    "        yield ('Hello', 'Hi'), True\n",
    "        # Next we have the second turn. This time, the last element is False, indicating we're still going\n",
    "        yield ('How are you', 'I am fine'), False\n",
    "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
    "        \n",
    "        # second episode. We need to have True again!\n",
    "        yield (\"Hey\", \"hi there\"), True\n",
    "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
    "        yield (\"Last chance\", \"This is it\"), False\n",
    "        \n",
    "        \n",
    "DisplayData.main(task=\"my_teacher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vvwxi6gXw8jU"
   },
   "source": [
    "Notice how the data corresponds to the utterances we provided? In reality, we'd normally want to load up a data file, loop through it, and yield the tuples from processed data. But for this simple example, it works well.\n",
    "\n",
    "We can now use our teacher in the standard places! Let's see how the model we trained earlier behaves with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "ZIyZQnxAw5HG",
    "outputId": "7240d04f-f9d3-4188-9bc8-c67f99b873b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ warning: overriding opt['task'] to my_teacher (previously: empathetic_dialogues )]\n",
      "[ warning: overriding opt['skip_generation'] to False (previously: True )]\n",
      "[ Using CUDA ]\n",
      "Dictionary: loading dictionary from from_pretrained/model.dict\n",
      "[ num words =  54944 ]\n",
      "Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "[ Loading existing model params from from_pretrained/model ]\n",
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from valid.txt ~~ \n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
      "\u001b[0mHello\u001b[0;0m\n",
      "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
      "\u001b[0;95m     model: hi\u001b[0;0m\n",
      "\u001b[0mHow are you\u001b[0;0m\n",
      "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
      "\u001b[0;95m     model: i am good , how are you ?\u001b[0;0m\n",
      "\u001b[0mLet's say goodbye\u001b[0;0m\n",
      "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
      "\u001b[0;95m     model: i am fine\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
      "\u001b[0mHey\u001b[0;0m\n",
      "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
      "\u001b[0;95m     model: hi\u001b[0;0m\n",
      "\u001b[0mDeja vu?\u001b[0;0m\n",
      "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
      "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
      "\u001b[0mLast chance\u001b[0;0m\n",
      "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
      "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
      "EPOCH DONE\n"
     ]
    }
   ],
   "source": [
    "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model', skip_generation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOzvSHAy0meK"
   },
   "source": [
    "Note that the `register_teacher` decorator makes the commands aware of your teacher. If you leave it off, the commands won't be able to locate it. If you want to use your teacher on the command line, you'll need to put it in a very specific filename: `parlai/agents/my_teacher/agents.py`, and you'll need to name the class `DefaultTeacher` instead of `MyTeacher`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJj7Lhs00oOB"
   },
   "source": [
    "# Creating your own models\n",
    "\n",
    "As a start, we'll implement a *very* simple agent. This agent will just sort of respond with \"hello X, my name is Y\", where X is based on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pykhtFDrxCPo"
   },
   "outputs": [],
   "source": [
    "from parlai.core.agents import register_agent, Agent\n",
    "\n",
    "@register_agent(\"hello\")\n",
    "class HelloAgent(Agent):\n",
    "    @classmethod\n",
    "    def add_cmdline_args(cls, parser, partial_opt):\n",
    "        parser.add_argument('--name', type=str, default='Alice', help=\"The agent's name.\")\n",
    "        return parser\n",
    "        \n",
    "    def __init__(self, opt, shared=None):\n",
    "        # similar to the teacher, we have the Opt and the shared memory objects!\n",
    "        super().__init__(opt, shared)\n",
    "        self.id = 'HelloAgent'\n",
    "        self.name = opt['name']\n",
    "    \n",
    "    def observe(self, observation):\n",
    "        # Gather the last word from the other user's input\n",
    "        words = observation.get('text', '').split()\n",
    "        if words:\n",
    "            self.last_word = words[-1]\n",
    "        else:\n",
    "            self.last_word = \"stranger!\"\n",
    "    \n",
    "    def act(self):\n",
    "        # Always return a string like this.\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'text': f\"Hello {self.last_word}, I'm {self.name}\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1SZmy_s0sGd"
   },
   "source": [
    "Let's try seeing how this agent behaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "HcS1UIFH0pb6",
    "outputId": "bcc52fbf-49f8-47ed-e5db-09e5b6c57b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from valid.txt ~~ \n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
      "\u001b[0mHello\u001b[0;0m\n",
      "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
      "\u001b[0;95m     model: Hello Hello, I'm Alice\u001b[0;0m\n",
      "\u001b[0mHow are you\u001b[0;0m\n",
      "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
      "\u001b[0;95m     model: Hello you, I'm Alice\u001b[0;0m\n",
      "\u001b[0mLet's say goodbye\u001b[0;0m\n",
      "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
      "\u001b[0;95m     model: Hello goodbye, I'm Alice\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
      "\u001b[0mHey\u001b[0;0m\n",
      "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
      "\u001b[0;95m     model: Hello Hey, I'm Alice\u001b[0;0m\n",
      "\u001b[0mDeja vu?\u001b[0;0m\n",
      "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
      "\u001b[0;95m     model: Hello vu?, I'm Alice\u001b[0;0m\n",
      "\u001b[0mLast chance\u001b[0;0m\n",
      "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
      "\u001b[0;95m     model: Hello chance, I'm Alice\u001b[0;0m\n",
      "EPOCH DONE\n"
     ]
    }
   ],
   "source": [
    "DisplayModel.main(task='my_teacher', model='hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmvcRSGS0wQE"
   },
   "source": [
    "Notice how it read the words from the user, and provides its name from the command line argument? We can also interact with it easily enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "_xd5CaG00tv6",
    "outputId": "4d9aee8c-d390-46c7-e685-febcd5a5b91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_examples: False ]\n",
      "[  display_ignore_fields: label_candidates,text_candidates ]\n",
      "[  display_prettify: False ]\n",
      "[  interactive_task: True ]\n",
      "[  name: Bob ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /usr/local/lib/python3.6/dist-packages/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /usr/local/lib/python3.6/dist-packages/downloads ]\n",
      "[  dynamic_batching: None ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  init_opt: None ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: interactive ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: hello ]\n",
      "[  model_file: None ]\n",
      "[ Local Human Arguments: ] \n",
      "[  local_human_candidates_file: None ]\n",
      "[  single_turn: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
      "[creating task(s): interactive]\n",
      "\u001b[0mEnter Your Message:\u001b[0;0m hi, who are you?\n",
      "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello you?, I'm Bob\u001b[0;0m\n",
      "\u001b[0mEnter Your Message:\u001b[0;0m My name is Stephen\n",
      "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello Stephen, I'm Bob\u001b[0;0m\n",
      "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
      "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello stranger!, I'm Bob\u001b[0;0m\n",
      "CHAT DONE \n"
     ]
    }
   ],
   "source": [
    "Interactive.main(model='hello', name='Bob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAe1hytf1BPk"
   },
   "source": [
    "Similar to the teacher, the call to `register_agent` makes it available for use in commands. If you forget the `register_agent` decorator, you won't be able to refer to it. Similarly, if you wanted to use this model from the command line, you would need to save this code to a special folder: `parlai/agents/hello/hello.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aBbhKTO1DEE"
   },
   "source": [
    "## Creating a neural network model\n",
    "\n",
    "The base Agent class is very simple, but it also provides extremely little functionality. We have created solid abstractions for creating neural-network type models. [`TorchGeneratorAgent`](https://parl.ai/docs/torch_agent.html#module-parlai.core.torch_generator_agent) is one our common abstractions, and it assumes a model which outputs one-word-at-a-time.\n",
    "\n",
    "The following is from our [ExampleSeq2Seq](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/examples/seq2seq.py) agent. It's a simple RNN model, trained like a Machine Translation model. The Model is too complex to go over in this document, but please feel free to [read our TorchGeneratorAgent tutorial](https://parl.ai/docs/tutorial_torch_generator_agent.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVrZh-T903wh"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import parlai.core.torch_generator_agent as tga\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Example encoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
    "    specified hidden size.\n",
    "    Pay particular attention to the ``forward`` output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        Arguments here can be used to provide hyperparameters.\n",
    "        \"\"\"\n",
    "        # must call super on all nn.Modules.\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        \"\"\"\n",
    "        Perform the forward pass for the encoder.\n",
    "        Input *must* be input_tokens, which are the context tokens given\n",
    "        as a matrix of lookup IDs.\n",
    "        :param input_tokens:\n",
    "            Input tokens as a bsz x seqlen LongTensor.\n",
    "            Likely will contain padding.\n",
    "        :return:\n",
    "            You can return anything you like; it is will be passed verbatim\n",
    "            into the decoder for conditioning. However, it should be something\n",
    "            you can easily manipulate in ``reorder_encoder_states``.\n",
    "            This particular implementation returns the hidden and cell states from the\n",
    "            LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embeddings(input_tokens)\n",
    "        _output, hidden = self.lstm(embedded)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic example decoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
    "    specified hidden size. Decoder allows for incremental decoding by ingesting the\n",
    "    current incremental state on each forward pass.\n",
    "    Pay particular note to the ``forward``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        Arguments here can be used to provide hyperparameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input, encoder_state, incr_state=None):\n",
    "        \"\"\"\n",
    "        Run forward pass.\n",
    "        :param input:\n",
    "            The currently generated tokens from the decoder.\n",
    "        :param encoder_state:\n",
    "            The output from the encoder module.\n",
    "        :parm incr_state:\n",
    "            The previous hidden state of the decoder.\n",
    "        \"\"\"\n",
    "        embedded = self.embeddings(input)\n",
    "        if incr_state is None:\n",
    "            # this is our very first call. We want to seed the LSTM with the\n",
    "            # hidden state of the decoder\n",
    "            state = encoder_state\n",
    "        else:\n",
    "            # We've generated some tokens already, so we can reuse the existing\n",
    "            # decoder state\n",
    "            state = incr_state\n",
    "\n",
    "        # get the new output and decoder incremental state\n",
    "        output, incr_state = self.lstm(embedded, state)\n",
    "\n",
    "        return output, incr_state\n",
    "\n",
    "\n",
    "class ExampleModel(tga.TorchGeneratorModel):\n",
    "    \"\"\"\n",
    "    ExampleModel implements the abstract methods of TorchGeneratorModel to define how to\n",
    "    re-order encoder states and decoder incremental states.\n",
    "    It also instantiates the embedding table, encoder, and decoder, and defines the\n",
    "    final output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dictionary, hidden_size=1024):\n",
    "        super().__init__(\n",
    "            padding_idx=dictionary[dictionary.null_token],\n",
    "            start_idx=dictionary[dictionary.start_token],\n",
    "            end_idx=dictionary[dictionary.end_token],\n",
    "            unknown_idx=dictionary[dictionary.unk_token],\n",
    "        )\n",
    "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
    "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
    "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
    "\n",
    "    def output(self, decoder_output):\n",
    "        \"\"\"\n",
    "        Perform the final output -> logits transformation.\n",
    "        \"\"\"\n",
    "        return F.linear(decoder_output, self.embeddings.weight)\n",
    "\n",
    "    def reorder_encoder_states(self, encoder_states, indices):\n",
    "        \"\"\"\n",
    "        Reorder the encoder states to select only the given batch indices.\n",
    "        Since encoder_state can be arbitrary, you must implement this yourself.\n",
    "        Typically you will just want to index select on the batch dimension.\n",
    "        \"\"\"\n",
    "        h, c = encoder_states\n",
    "        return h[:, indices, :], c[:, indices, :]\n",
    "\n",
    "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
    "        \"\"\"\n",
    "        Reorder the decoder states to select only the given batch indices.\n",
    "        This method can be a stub which always returns None; this will result in the\n",
    "        decoder doing a complete forward pass for every single token, making generation\n",
    "        O(n^2). However, if any state can be cached, then this method should be\n",
    "        implemented to reduce the generation complexity to O(n).\n",
    "        \"\"\"\n",
    "        h, c = incr_state\n",
    "        return h[:, indices, :], c[:, indices, :]\n",
    "\n",
    "\n",
    "@register_agent(\"my_first_lstm\")\n",
    "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
    "    \"\"\"\n",
    "    Example agent.\n",
    "    Implements the interface for TorchGeneratorAgent. The minimum requirement is that it\n",
    "    implements ``build_model``, but we will want to include additional command line\n",
    "    parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def add_cmdline_args(cls, argparser, partial_opt):\n",
    "        \"\"\"\n",
    "        Add CLI arguments.\n",
    "        \"\"\"\n",
    "        # Make sure to add all of TorchGeneratorAgent's arguments\n",
    "        super().add_cmdline_args(argparser)\n",
    "\n",
    "        # Add custom arguments only for this model.\n",
    "        group = argparser.add_argument_group('Example TGA Agent')\n",
    "        group.add_argument(\n",
    "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
    "        )\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Construct the model.\n",
    "        \"\"\"\n",
    "\n",
    "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
    "        # Optionally initialize pre-trained embeddings by copying them from another\n",
    "        # source: GloVe, fastText, etc.\n",
    "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfR9w_Hm1HHY"
   },
   "source": [
    "Of course, now we can train with our new model. Let's train it on our toy task that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "SJMXpogz1E-_",
    "outputId": "3fe8275b-a7cd-491a-a761-fbd5849e2a7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dictionary: 100%|██████████| 6.00/6.00 [00:00<00:00, 1.91kex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ building dictionary first... ]\n",
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from train.txt ~~ \n",
      " ~~ Loading from train.txt ~~ \n",
      "Dictionary: saving dictionary to my_first_lstm/model.dict\n",
      "[ dictionary built with 30 tokens in 0s ]\n",
      "[ no model with opt yet at: my_first_lstm/model(.opt) ]\n",
      "[ Using CUDA ]\n",
      "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
      "[ num words =  30 ]\n",
      "Total parameters: 16,824,320 (16,824,320 trainable)\n",
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from train.txt ~~ \n",
      "[ training... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
      "     clip  exs  gnorm  gpu_mem   loss  lr   ppl  token_acc  total_train_updates   tpb  updates\n",
      "   .01641 1828  1.368    .9171 .04105   1 1.042      .9942                 1828 3.328     1828\n",
      "\n",
      "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
      "    gpu_mem  lr  total_train_updates\n",
      "      .9171   1                 1828\n",
      "\n",
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from valid.txt ~~ \n",
      "[ running eval: valid ]\n",
      "[ eval completed in 0.06s ]\n",
      "valid:\n",
      "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
      "           1 .0003337    6   1    .9171     0   1    1          1                 1828 3.333\n",
      "\n",
      "[ new best accuracy: 1 ]\n",
      "[ saving best valid model: my_first_lstm/model ]\n",
      "[ task solved! stopping. ]\n",
      "[ Using CUDA ]\n",
      "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
      "[ num words =  30 ]\n",
      "Total parameters: 16,824,320 (16,824,320 trainable)\n",
      "[ Loading existing model params from my_first_lstm/model ]\n",
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from valid.txt ~~ \n",
      "[ running eval: valid ]\n",
      "[ eval completed in 0.05s ]\n",
      "valid:\n",
      "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
      "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
      "\n",
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from test.txt ~~ \n",
      "[ running eval: test ]\n",
      "[ eval completed in 0.05s ]\n",
      "test:\n",
      "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
      "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# of course, we can train the model! Let's Train it on our silly toy task from above\n",
    "!rm -rf my_first_lstm\n",
    "!mkdir -p my_first_lstm\n",
    "\n",
    "TrainModel.main(\n",
    "    model='my_first_lstm',\n",
    "    model_file='my_first_lstm/model',\n",
    "    task='my_teacher',\n",
    "    batchsize=1,\n",
    "    validation_every_n_secs=10,\n",
    "    max_train_time=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hHrruVd1KnK"
   },
   "source": [
    "Let's see how it does. It should reproduce the data perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "shqFpdrE1Iif",
    "outputId": "1a89571c-2a42-48b8-9752-dffc4a58e557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using CUDA ]\n",
      "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
      "[ num words =  30 ]\n",
      "Total parameters: 16,824,320 (16,824,320 trainable)\n",
      "[ Loading existing model params from my_first_lstm/model ]\n",
      "[creating task(s): my_teacher]\n",
      " ~~ Loading from valid.txt ~~ \n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
      "\u001b[0mHello\u001b[0;0m\n",
      "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
      "\u001b[0;95m     model: Hi\u001b[0;0m\n",
      "\u001b[0mHow are you\u001b[0;0m\n",
      "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
      "\u001b[0;95m     model: I am fine\u001b[0;0m\n",
      "\u001b[0mLet's say goodbye\u001b[0;0m\n",
      "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
      "\u001b[0;95m     model: Goodbye !\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
      "\u001b[0mHey\u001b[0;0m\n",
      "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
      "\u001b[0;95m     model: hi there\u001b[0;0m\n",
      "\u001b[0mDeja vu?\u001b[0;0m\n",
      "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
      "\u001b[0;95m     model: Deja vu !\u001b[0;0m\n",
      "\u001b[0mLast chance\u001b[0;0m\n",
      "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
      "\u001b[0;95m     model: This is it\u001b[0;0m\n",
      "EPOCH DONE\n"
     ]
    }
   ],
   "source": [
    "DisplayModel.main(model_file='my_first_lstm/model', task='my_teacher')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hzrDhPS1QCW"
   },
   "source": [
    "Unsurprisingly, we got perfect accuracy. This is because the data set is only a handful of utterances, and we can perfectly memorize it in this LSTM. Nonetheless, a great success!\n",
    "\n",
    "# What's next!\n",
    "\n",
    "The sky's the limit! Be sure to check out our [GitHub](https://github.com/facebookresearch/ParlAI) and [Follow ParlAI on Twitter](https://twitter.com/parlai_parley). We're eager to hear what you are using ParlAI for!\n",
    "\n",
    "Here are some other great resources:\n",
    "- [Our research page](https://parl.ai/projects/)\n",
    "- [ParlAI Documentations](https://parl.ai/docs/index.html)\n",
    "- [Tutorial: Writing a Ranker model](https://parl.ai/docs/tutorial_torch_ranker_agent.html)\n",
    "- [Tutorial: Using Mechanical Turk](https://parl.ai/docs/tutorial_mturk.html)\n",
    "- [Tutorial: Connecting to chat services](https://parl.ai/docs/tutorial_chat_service.html)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "parlai_3.10",
   "language": "python",
   "name": "parlai_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
